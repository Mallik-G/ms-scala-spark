<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type" />
<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible" />
<meta content="Asciidoctor 1.5.2" name="generator" />
<title>Spark Streaming</title>
<link href="deck.js/themes/style/font.css" rel="stylesheet" />
<style>
.conum { display: inline-block; color: white !important; background-color: #222222; -webkit-border-radius: 100px; border-radius: 100px; text-align: center; width: 1.2em; height: 1.2em; font-size: 0.9em; font-weight: bold; line-height: 1.2; font-family: Arial, sans-serif; font-style: normal; position: relative; top: -0.1em; }
.conum * { color: white !important; }
.conum + b { display: none; }
.conum:after { content: attr(data-value); }
.conum:not([data-value]):empty { display: none; }
.colist table td:first-of-type { padding-right: 0.25em; }
</style>
<style>
/* Stylesheet for CodeRay to match GitHub theme | MIT License | http://foundation.zurb.com */
/*pre.CodeRay {background-color:#f7f7f8;}*/
.CodeRay .line-numbers{border-right:1px solid #d8d8d8;padding:0 0.5em 0 .25em}
.CodeRay span.line-numbers{display:inline-block;margin-right:.5em;color:rgba(0,0,0,.3)}
.CodeRay .line-numbers strong{font-weight: normal}
table.CodeRay{border-collapse:separate;border-spacing:0;margin-bottom:0;border:0;background:none}
table.CodeRay td{vertical-align: top}
table.CodeRay td.line-numbers{text-align:right}
table.CodeRay td.line-numbers>pre{padding:0;color:rgba(0,0,0,.3)}
table.CodeRay td.code{padding:0 0 0 .5em}
table.CodeRay td.code>pre{padding:0}
.CodeRay .debug{color:#fff !important;background:#000080 !important}
.CodeRay .annotation{color:#007}
.CodeRay .attribute-name{color:#000080}
.CodeRay .attribute-value{color:#700}
.CodeRay .binary{color:#509}
.CodeRay .comment{color:#998;font-style:italic}
.CodeRay .char{color:#04d}
.CodeRay .char .content{color:#04d}
.CodeRay .char .delimiter{color:#039}
.CodeRay .class{color:#458;font-weight:bold}
.CodeRay .complex{color:#a08}
.CodeRay .constant,.CodeRay .predefined-constant{color:#008080}
.CodeRay .color{color:#099}
.CodeRay .class-variable{color:#369}
.CodeRay .decorator{color:#b0b}
.CodeRay .definition{color:#099}
.CodeRay .delimiter{color:#000}
.CodeRay .doc{color:#970}
.CodeRay .doctype{color:#34b}
.CodeRay .doc-string{color:#d42}
.CodeRay .escape{color:#666}
.CodeRay .entity{color:#800}
.CodeRay .error{color:#808}
.CodeRay .exception{color:inherit}
.CodeRay .filename{color:#099}
.CodeRay .function{color:#900;font-weight:bold}
.CodeRay .global-variable{color:#008080}
.CodeRay .hex{color:#058}
.CodeRay .integer,.CodeRay .float{color:#099}
.CodeRay .include{color:#555}
.CodeRay .inline{color:#00}
.CodeRay .inline .inline{background:#ccc}
.CodeRay .inline .inline .inline{background:#bbb}
.CodeRay .inline .inline-delimiter{color:#d14}
.CodeRay .inline-delimiter{color:#d14}
.CodeRay .important{color:#555;font-weight:bold}
.CodeRay .interpreted{color:#b2b}
.CodeRay .instance-variable{color:#008080}
.CodeRay .label{color:#970}
.CodeRay .local-variable{color:#963}
.CodeRay .octal{color:#40e}
.CodeRay .predefined{color:#369}
.CodeRay .preprocessor{color:#579}
.CodeRay .pseudo-class{color:#555}
.CodeRay .directive{font-weight:bold}
.CodeRay .type{font-weight:bold}
.CodeRay .predefined-type{color:inherit}
.CodeRay .reserved,.CodeRay .keyword {color:#000;font-weight:bold}
.CodeRay .key{color:#808}
.CodeRay .key .delimiter{color:#606}
.CodeRay .key .char{color:#80f}
.CodeRay .value{color:#088}
.CodeRay .regexp .delimiter{color:#808}
.CodeRay .regexp .content{color:#808}
.CodeRay .regexp .modifier{color:#808}
.CodeRay .regexp .char{color:#d14}
.CodeRay .regexp .function{color:#404;font-weight:bold}
.CodeRay .string{color:#d20}
.CodeRay .string .string .string{background:#ffd0d0}
.CodeRay .string .content{color:#d14}
.CodeRay .string .char{color:#d14}
.CodeRay .string .delimiter{color:#d14}
.CodeRay .shell{color:#d14}
.CodeRay .shell .delimiter{color:#d14}
.CodeRay .symbol{color:#990073}
.CodeRay .symbol .content{color:#a60}
.CodeRay .symbol .delimiter{color:#630}
.CodeRay .tag{color:#008080}
.CodeRay .tag-special{color:#d70}
.CodeRay .variable{color:#036}
.CodeRay .insert{background:#afa}
.CodeRay .delete{background:#faa}
.CodeRay .change{color:#aaf;background:#007}
.CodeRay .head{color:#f8f;background:#505}
.CodeRay .insert .insert{color:#080}
.CodeRay .delete .delete{color:#800}
.CodeRay .change .change{color:#66f}
.CodeRay .head .head{color:#f4f}
</style>
<link href="deck.js/core/deck.core.css" rel="stylesheet" />
<link href="deck.js/extensions/scale/deck.scale.css" media="screen" rel="stylesheet" />
<link href="deck.js/themes/style/datastax.css" media="screen" rel="stylesheet" />
<link href="deck.js/themes/transition/fade.css" media="screen" rel="stylesheet" />
<link href="deck.js/core/print.css" media="print" rel="stylesheet" />
<script src="deck.js/modernizr.custom.js"></script>
</head>
<body class="article">
<div class="deck-container">
<section class="slide" id="title-slide">
<h1>Spark Streaming</h1>
</section>
<section class="slide" id="spark-streaming-dstream">
<h2>Stream Processing</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="stream processing system" src="images/spark/streaming/dstream/stream-processing-system.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is a general stream processing system.</p></div>
<div class="ulist">
<ul>
<li><p>
Data arrives in one or more streams<div class="ulist">
<ul>
<li>Sensor data and time series</li>
<li>Internet and Web traffic</li>
<li>Events, interactions, and transactions</li>
<li>Surveillance data</li>
</ul>
</div></p></li>
<li><p>
Data stream needs to be processed immediately (in real time)<div class="ulist">
<ul>
<li>Basic activity statistics and monitoring</li>
<li>Insights, trends, recommendations</li>
<li>Anomaly, spam, fraud, and intrusion detection</li>
</ul>
</div></p></li>
<li><p>
It may be too expensive or unnecessary to store raw data<div class="ulist">
<ul>
<li>The archival storage may be optional but it is indeed useful for off-line analytics</li>
<li>Storing a summary (aggregates) of data may be sufficient in some cases</li>
</ul>
</div></p></li>
<li><p>
What happens inside the stream processor may vary:<div class="ulist">
<ul>
<li><p>
Record-at-a-time processing (traditional approach)<div class="ulist">
<ul>
<li>Storm (Twitter)</li>
<li>Samza (LinkedIn)</li>
<li>S4 (Yahoo)</li>
<li>MillWheel (Google)</li>
</ul>
</div></p></li>
<li><p>
Micro-batch computation on small time intervals<div class="ulist">
<ul>
<li>Spark Streaming</li>
<li>Some of the traditional systems now support micro-batching, too</li>
</ul>
</div></p></li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming">
<h2>Spark Streaming</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="spark streaming" src="images/spark/streaming/dstream/spark-streaming.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li><p>
Spark Streaming + Cassandra<div class="ulist">
<ul>
<li>Spark Streaming = Stream Processor</li>
<li>Cassandra = Storage (both working and archival when applicable)</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="paragraph"><p>Spark supports a number of <em>input stream sources</em>. Each stream is served by
exactly one <em>Receiver</em> that "knows" how to deal with a specific source. It is even
possible to create custom <em>Receivers</em> to support additional stream sources.</p></div>
<div class="paragraph"><p><em>Receiver</em> combines records from a specific time interval (e.g., a few seconds) into a micro-batch
and gives each micro-batch to the Spark engine for processing.
The stream of micro-batches is called <em>Discretized Stream</em>.</p></div>
<div class="paragraph"><p>The Spark engine uses <em>Spark-Cassandra Connector</em>
to interact with Cassandra.</p></div>
</div>
</div>
</section>
<section class="slide" id="discretized-stream-dstream">
<h2>Discretized Stream&#8201;&#8212;&#8201;DStream</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="dstream" src="images/spark/streaming/dstream/dstream.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here is a closer look at the <em>Discretized Stream</em> or simply <em>DStream</em>.</p></div>
<div class="ulist">
<ul>
<li>Data stream is represented as Discretized Stream (DStream)</li>
<li>Data stream is divided into small batches of data</li>
<li>Micro-batches of data are created at regular time intervals</li>
<li>Batch interval or time step is typically between 500ms and several seconds</li>
<li>Micro-batches can be of different sizes, depending on how much data arrives at each time interval</li>
<li>Each micro-batch is represented as Resilient Distributed Dataset (RDD)</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="dstream-transformation">
<h2>DStream Transformation</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="dstream transformation" src="images/spark/streaming/dstream/dstream-transformation.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li><em>DStream</em> is a continuous sequence of RDDs!</li>
<li>Each RDD in a DStream has lineage and fault-tolerance</li>
<li>Transformations similar to those on generic and key-value pair RDDs are applicable</li>
<li>Such transformations are called on a <em>DStream</em> and applied on each constituent RDD individually</li>
<li>There are many additional transformations and output operations that are only applicable to discretized streams, including some
that work on data from multiple RDDs in a <em>DStream</em></li>
</ul>
</div>
<div class="paragraph"><p>This example shows the result of applying the <em>filter</em> transformation on a <em>DStream</em>. It is like applying <em>filter</em>
on each RDD in the <em>DStream</em>. Note that the result is a new <em>Discretized Stream</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-architecture">
<h2>Spark Streaming Architecture</h2>
<div class="imageblock" style="float: center">
<div class="content">
<img alt="streaming architecture" src="images/spark/streaming/architecture/streaming-architecture.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>You should already be familiar with Spark Architecture:</p></div>
<div class="ulist">
<ul>
<li><em>Client</em>, <em>Driver</em>, <em>SparkContext</em></li>
<li><em>Master</em></li>
<li><em>Worker</em></li>
<li><em>Executor</em></li>
</ul>
</div>
<div class="paragraph"><p>Spark Streaming Architecture has additional components:</p></div>
<div class="ulist">
<ul>
<li><p>
<em>StreamingContext</em><div class="ulist">
<ul>
<li>Entry point to Spark Streaming functionality</li>
<li>Uses <em>SparkContext</em></li>
</ul>
</div></p></li>
<li><p>
<em>Data Stream Source</em><div class="ulist">
<ul>
<li>Spark supports many sources that are named on the next slide</li>
</ul>
</div></p></li>
<li><p>
<em>Receiver</em><div class="ulist">
<ul>
<li>“Long task” (constantly running task) that belongs to one of the <em>Executors</em></li>
<li>Creates a <em>DStream</em> from an input data stream</li>
<li>Replicates a <em>DStream</em> to the cache of another <em>Worker</em>--<em>Executor</em> by default</li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="data-stream-sources">
<h2>Data Stream Sources</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:33%" />
<col style="width:33%" />
<col style="width:33%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Basic Sources</th>
<th class="tableblock halign-left valign-top">Advanced Sources</th>
<th class="tableblock halign-left valign-top">Custom Sources</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>File systems</li>
<li>Socket connections</li>
<li>Akka Actors</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>Kafka</li>
<li>Flume</li>
<li>Kinesis</li>
<li>Twitter</li>
<li>ZeroMQ</li>
<li>MQTT</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>User-defined receivers</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Basic Sources are available through <em>StreamingContext</em> API.</p></div>
<div class="paragraph"><p>Advanced sources are available through additional libraries (utility classes).</p></div>
<div class="paragraph"><p>Spark allows extending support to custom sources as well.</p></div>
</div>
</div>
</section>
<section class="slide" id="receiver-reliability">
<h2>Receiver Reliability</h2>
<div class="ulist">
<ul>
<li><p>
Reliable Receiver<div class="ulist">
<ul>
<li>Acknowledges a reliable source that data has been received and replicated</li>
<li>A reliable source replicates its own data and is able to resend it</li>
</ul>
</div></p></li>
<li><p>
Unreliable Receiver<div class="ulist">
<ul>
<li>Does not acknowledge a source</li>
<li>Data loss is possible</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>There is exactly one <em>Receiver</em> per stream.
Different data stream sources have different receivers.</p></div>
<div class="paragraph"><p>This is a classification of receivers based on reliability.</p></div>
<div class="paragraph"><p>Note that, even if a source is reliable (can accept acknowledgments, replicates its data, can resend its data),
its corresponding receiver can be designed to be unreliable (to avoid extra complexity) when data loss is not a problem.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-first-application">
<h2>The WordCount Problem in a Stream</h2>
<div class="imageblock center">
<div class="content">
<img alt="tag cloud" src="images/spark/streaming/first-application/tag-cloud.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate the basics of Spark Streaming, we will count words in a stream.
This problem is indeed similar to the classic <em>WordCount</em> problem except
that counting is done on micro-batches of records collected over a small time interval.
While the problem seems simple, it can be useful to assess the current trend.</p></div>
<div class="paragraph"><p>Let us assume that we are dealing with a stream of genres generated by users who access
movies on our <em>KillrVideo</em> website.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Application design steps</strong></p></div>
<div class="olist arabic">
<ol class="arabic">
<li>
Import classes
</li>
<li>
Initialize a <em>SparkConf</em> object
</li>
<li>
Initialize <em>StreamingContext</em> and <em>SparkContext</em>
</li>
<li>
Create a <em>DStream</em> object
</li>
<li>
Define computation
</li>
<li>
Start computation
</li>
</ol>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These are steps we will take to design the application.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-1-importing-classes">
<h2>Step 1: Importing Classes</h2>
<div class="paragraph"><p><strong>Necessary imports</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>// Spark
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._

// Spark Streaming
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._

// Spark-Cassandra Connector
import com.datastax.spark.connector._
import com.datastax.spark.connector.streaming._</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Step 1 imports classes that are needed to use:</p></div>
<div class="ulist">
<ul>
<li>Spark Core API</li>
<li>Spark Streaming API</li>
<li>Spark-Cassandra Connector API</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="step-2-initializing-a-sparkconf-object">
<h2>Step 2: Initializing a SparkConf Object</h2>
<div class="paragraph"><p><strong>Connection configuration and execution properties</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val conf = new SparkConf(true)
      .setAppName("Streaming Example")
      .setMaster("spark://127.0.0.1:7077")
      .set("spark.cassandra.connection.host", "127.0.0.1")
      .set("spark.cleaner.ttl", "3600")
      .setJars(Array("your-app.jar"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>We create a Spark configuration object and set the following properties:</p></div>
<div class="olist arabic">
<ol class="arabic">
<li>
Application name
</li>
<li>
Spark Master URL
</li>
<li>
A Cassandra host (can be any node in a Cassandra cluster as they all are peers)
</li>
<li>
<p><em>spark.cleaner.ttl</em></p>
<div class="ulist">
<ul>
<li>Duration (seconds) of how long Spark will remember any metadata (stages, tasks, cached RDDs)</li>
<li>Spark stores metadata for the duration of application lifetime; if our streaming application
runs for many weeks, months and so forth, we want to limit how much metadata is stored</li>
</ul>
</div>
</li>
<li>
An application JAR file to distribute to a cluster
</li>
</ol>
</div>
<div class="paragraph"><p>There are many other configurable properties for Applications, Spark, Spark Streaming, and Spark-Cassandra Connector.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-3-initializing-streamingcontext-and-sparkcontext">
<h2>Step 3: Initializing StreamingContext and SparkContext</h2>
<div class="paragraph"><p><strong>Two alternative ways</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))
val sc  = ssc.sparkContext</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val sc  = new SparkContext(conf)
val ssc = new StreamingContext(sc, Seconds(4))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>SparkContext</em> is an entry point for Spark functionality.</p></div>
<div class="paragraph"><p><em>StreamingContext</em> is an entry point for Spark Streaming functionality.</p></div>
<div class="paragraph"><p>Approach 1:</p></div>
<div class="ulist">
<ul>
<li>Creating a <em>StreamingContext</em> object for the <em>SparkConf</em> object and the batch interval of 4 seconds</li>
<li>An underlying <em>SparkContext</em> object is created automatically and can be accessed via property <em>ssc.sparkContext</em></li>
</ul>
</div>
<div class="paragraph"><p>Approach 2:</p></div>
<div class="ulist">
<ul>
<li>Creating a <em>SparkContext</em> object for the <em>SparkConf</em> object</li>
<li>Creating a <em>StreamingContext</em> object for the <em>SparkContext</em> object and the batch interval of 4 seconds</li>
</ul>
</div>
<div class="paragraph"><p>Choosing between these two approaches is a matter of preference.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-4-creating-a-dstream-object">
<h2>Step 4: Creating a DStream Object</h2>
<div class="paragraph"><p><strong>Receiving stream data via a TCP socket</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val stream = ssc.socketTextStream("127.0.0.1", 9999)</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>The simplest and quickest way to setup a socket data stream for testing purposes is
to run <code>nc -lk 9999</code> in a separate terminal and type in records manually. It is also relatively
straightforward to write a custom shell script that generates stream data.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Depending on a data stream source, you may have to use different API calls. For advanced sources, such as
Kafka, Flume, Kinesis, Twitter, ZeroMQ, and MQTT, additional libraries are required.</p></div>
<div class="paragraph"><p>In our simple example, we are creating a <em>DStream</em> object that receives data via a TCP socket defined by a host and a port.</p></div>
<div class="paragraph"><p>You can create multiple streams (<em>DStream</em> objects) as needed.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-5-defining-computation">
<h2>Step 5: Defining Computation</h2>
<div class="paragraph"><p><strong>Counting words in micro-batches</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>stream.flatMap(record =&gt; record.split(" "))
      .map(word =&gt; (word,1))
      .reduceByKey(_ + _)
      .print()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This is the most interesting part.</p></div>
<div class="paragraph"><p>This code transforms the input stream of micro-batches using
<em>flatMap</em> into a new stream where RDD elements are individual words.
It then maps words to key-value pairs and counts words using transformation <em>reduceByKey</em> resulting in a new <em>DStream</em>.
Finally, it outputs the first 10 elements of each micro-batch RDD to the screen using output operation <em>print</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="step-6-starting-computation">
<h2>Step 6: Starting Computation</h2>
<div class="paragraph"><p><strong>Computation will be terminated manually or due to an error</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>ssc.start()

ssc.awaitTermination()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Once you have the stream source and computation defined,
this is how you start computation and wait for its termination, which can be manual or due to an error.</p></div>
</div>
</div>
</section>
<section class="slide" id="complete-example">
<h2>Complete Example</h2>
<div class="paragraph"><p><strong>StreamingExample.scala</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
import com.datastax.spark.connector._
import com.datastax.spark.connector.streaming._

object StreamingExample {
  def main(args: Array[String]) {

     val conf = new SparkConf(true)
        .setAppName("Streaming Example")
        .setMaster("spark://127.0.0.1:7077")
        .set("spark.cassandra.connection.host", "127.0.0.1")
        .set("spark.cleaner.ttl", "3600")
        .setJars(Array("/…/target/scala-2.10/streaming-example_2.10-0.1.jar"))

     val ssc = new StreamingContext(conf, Seconds(4))

     val stream = ssc.socketTextStream("127.0.0.1", 9999)

     stream.flatMap(record =&gt; record.split(" "))
           .map(word =&gt; (word,1))
           .reduceByKey(_ + _)
           .print()

     ssc.start()
     ssc.awaitTermination()
  }
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here is a complete and very simple Spark Streaming application.</p></div>
<div class="paragraph"><p>Of course, the hard-coded values for Spark Master, Cassandra node, and stream socket should rather
be passed as parameters to the main method.</p></div>
<div class="paragraph"><p>This Scala file should be placed into a <em>src</em> folder of a project. The project can be compiled and packaged using
SBT (Scala/Simple Build Tool). The jar package is then submitted to the DSE cluster using <em>dse spark-submit</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="sample-output">
<h2>Sample Output</h2>
<div class="paragraph"><p><strong>Results of processing two micro-batches</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>-------------------------------------------
Time: 1443548140000 ms
-------------------------------------------
(Drama,22)
(Comedy,12)
(Action,16)
(Romance,9)
(Dance,11)
(Family,18)
(Adventure,7)
(Horror,10)
(Thriller,19)
(Fantasy,11)
...

-------------------------------------------
Time: 1443548144000 ms
-------------------------------------------
(Drama,11)
(Comedy,14)
(Action,13)
(Romance,2)
(Dance,9)
(Family,12)
(Adventure,9)
(Science,2)
(Thriller,3)
(Fantasy,8)
...</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>print</em> outputs first 10 elements from each micro-batch. The output is not sorted.</p></div>
<div class="paragraph"><p>Notice that the batch timestamps differ by 4000 ms or 4 seconds.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-stateless-transformations">
<h2>Operations on DStreams</h2>
<div class="paragraph"><p><strong>Two types of DStream operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li><strong>Stateless transformations</strong></li>
<li>Stateful transformations</li>
</ul>
</div></p></li>
<li><em>Output operations</em></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on DStreams&#8201;&#8212;&#8201;transformations and output operations.</p></div>
<div class="paragraph"><p>Transformations transform input DStreams into new DStreams. They are evaluated lazily.
There are two types of DStream transformations: stateless and stateful.</p></div>
<div class="paragraph"><p>Output operations trigger computation. Their results are printed to the screen or
written to a stable storage.</p></div>
<div class="paragraph"><p>This presentation focuses on common stateless transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="how-do-stateless-transformations-work">
<h2>How Do Stateless Transformations Work?</h2>
<div class="paragraph"><p><strong>Transformation of a batch does not depend on any other batch in a DStream</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stateless transformation" src="images/spark/streaming/dstream-stateless-transformations/stateless-transformation.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Transformation of a batch does not depend on any other batch in a DStream.</p></div>
<div class="paragraph"><p>The illustration is for a unary stateless transformation. It shows a one-to-one correspondence for
input and output batches.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformations-on-dstreams-of-generic-rdds">
<h2>Transformations on DStreams of Generic RDDs</h2>
<div class="paragraph"><p><strong>Basic transformations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>filter</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by selecting those elements of the source DStream
on which a function <em>f</em> returns <em>true</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>map</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by applying a function <em>f</em> on each element of
the source DStream. There is a <em>one-to-one</em> correspondence between input and output elements.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>flatMap</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by applying a function <em>f</em> on each element of
the source DStream. There is a <em>one-to-many</em> correspondence between input and output elements
if <em>f</em> returns a <em>Seq</em> with more than one element.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>count</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs with a single element representing a number of elements
in respective RDDs of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>countByValue</strong>([<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by counting each distinct value in the source DStream.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduce</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs with a single element representing an aggregate value
computed by a function <em>f</em> from elements
in respective RDDs of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>union</strong>(<em>otherDStream</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed taking a union of respective RDDs in the source DStream and <em>otherDStream</em>.
The source DStream and <em>otherDStream</em> must agree on time.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These transformations on DStreams of generic RDDs should be familiar as there are similar operations (transformations or actions) on RDDs.</p></div>
<div class="paragraph"><p>Each batch is processed independently from other batches in an input DStream by these transformations.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Advanced transformations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>transform</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by applying an RDD-to-RDD function <em>f</em> to every RDD of the source DStream.
The function can apply any RDD transformations available in Spark, as well as custom RDD transformations.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>transformWith</strong>(<em>otherDStream</em>,<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A binary version of <em>transform</em> that works on two input DStreams.
The source DStream and <em>otherDStream</em> must agree on time.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These are DStream transformations that are very generic and powerful.</p></div>
<div class="paragraph"><p>Not all Spark Core transformations on RDDs have equivalent Spark Streaming transformations on DStreams.
This is when <em>transform</em> and <em>transformWith</em> shine!</p></div>
<div class="paragraph"><p>For example, we can use <em>transform</em> to join each RDD in a DStream with a Cassandra table.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-joining-dstream-and-cassandra-table">
<h2>Example: Joining DStream and Cassandra Table</h2>
<div class="paragraph"><p><strong>Augmenting a DStream with data stored in Cassandra</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="joining" src="images/spark/streaming/dstream-stateless-transformations/joining.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate a couple of the discussed transformations, let us solve the following problem.</p></div>
<div class="paragraph"><p>Given an input stream of movie identifiers (UUIDs), we need to count how many times each movie
appeared in a 4-second batch and join the result with Cassandra table <em>movies</em> to add
movie titles and years to a transformed DStream.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Using <em>countByValue</em> and <em>transform</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)
                .countByValue()

val movies = ssc.cassandraTable("killr_video","movies")
                .select("movie_id","title","release_year")
                .as( (id:UUID, t:String, y:Int)=&gt;(id.toString(),(t,y)) )
                .partitionBy(new HashPartitioner(2*ssc.sparkContext.defaultParallelism))
                .cache


stream.transform(rdd =&gt; rdd.join(movies).map{case(id,(c,(t,y))) =&gt; (id,t,y,c)})
      .print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Counting is done by transformation <em>countByValue</em>.</p></div>
<div class="paragraph"><p>RDD <em>movies</em> is pre-partitioned and cached because it is used in many joins with different RDD batches in the DStream.</p></div>
<div class="paragraph"><p>The join is performed inside of <em>transform</em>.</p></div>
<div class="paragraph"><p>Finally, output operation <em>print</em> triggers computation and displays results to the screen.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformations-on-dstreams-of-pair-rdds">
<h2>Transformations on DStreams of Pair RDDs</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:50%" />
<col style="width:50%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>mapValues</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top" rowspan="5"><p class="tableblock">A new DStream is formed by applying the corresponding transformation to each Pair RDD of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>flatMapValues</strong>(<em>f</em>)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByKey</strong>(<em>f</em>,[<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>groupByKey</strong>([<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>combineByKey</strong>(
<em>createCombinerF</em>,
<em>mergeValueF</em>,
<em>mergeCombinersF</em>,
[<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cogroup</strong>(<em>otherDStream</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top" rowspan="4"><p class="tableblock">A new DStream is formed by applying the corresponding binary transformation on
Pair RDDs from the source DStream and  <em>otherDStream</em>. The source DStream and <em>otherDStream</em> must agree on time.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>join</strong>(<em>otherDStream</em>, [<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>leftOuterJoin</strong>(<em>otherDStream</em>, [<em>numTasks</em>])</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>rightOuterJoin</strong>(<em>otherDStream</em>, [<em>numTasks</em>])</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These transformations on DStreams of Key-Value Pair RDDs should be familiar as there are similar operations (transformations or actions) on Pair RDDs.</p></div>
<div class="paragraph"><p>Each batch is processed independently from other batches in an input DStream by these transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-joining-two-dstreams">
<h2>Example: Joining Two DStreams</h2>
<div class="paragraph"><p><strong>Aggregating data from two DStreams</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="joining streams" src="images/spark/streaming/dstream-stateless-transformations/joining-streams.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate a couple of the discussed transformations, let us solve the following problem.</p></div>
<div class="paragraph"><p>We have two streams that are coming either from different sources or from the same source but the stream
is split into two streams to have better scalability with two receivers.</p></div>
<div class="paragraph"><p>Given two input streams of movie identifiers (UUIDs), we need to pre-aggregate how many times each movie
appeared in a 4-second batch, join respective batches from the streams, and do final aggregation (summation).</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Using <em>countByValue</em>, <em>join</em> and <em>mapValues</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream1 = ssc.socketTextStream(streamHost, 9999)
val stream2 = ssc.socketTextStream(streamHost, 9989)

stream1.countByValue()
       .join(stream2.countByValue())
       .mapValues{case (v1,v2) =&gt; v1 + v2}
       .print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Counting is done by transformation <em>countByValue</em>, resulting in Pair RDDs.</p></div>
<div class="paragraph"><p>The join is performed by transformation <em>join</em>.</p></div>
<div class="paragraph"><p>Summation of counts from two streams is done inside of transformation <em>mapValues</em>.</p></div>
<div class="paragraph"><p>Finally, output operation <em>print</em> triggers computation and displays results to the screen.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-stateful-transformations">
<h2>Operations on DStreams</h2>
<div class="paragraph"><p><strong>Two types of DStream operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li>Stateless transformations</li>
<li><p>
Stateful transformations<div class="ulist">
<ul>
<li><strong>Transformation <em>updateStateByKey</em></strong></li>
<li>Window transformations</li>
</ul>
</div></p></li>
</ul>
</div></p></li>
<li><em>Output operations</em></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on DStreams&#8201;&#8212;&#8201;transformations and output operations.</p></div>
<div class="paragraph"><p>Transformations transform input DStreams into new DStreams. They are evaluated lazily.
There are two types of DStream transformations: stateless and stateful.</p></div>
<div class="paragraph"><p>Stateful transformations include <em>updateStateByKey</em> and window transformations.</p></div>
<div class="paragraph"><p>Output operations trigger computation. Their results are printed to the screen or
written to a stable storage.</p></div>
<div class="paragraph"><p>This presentation focuses on stateful transformation <em>updateStateByKey</em>. Stateful window transformations
will be discussed in a separate presentation.</p></div>
</div>
</div>
</section>
<section class="slide" id="stateful-transformations">
<h2>Stateful Transformations</h2>
<div class="paragraph"><p><strong>Maintaining a "state" to combine data across multiple batches in a DStream</strong></p></div>
<div class="ulist">
<ul>
<li>Transformation <em>updateStateByKey</em></li>
<li>Window transformations (discussed separately)</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Stateful transformations have to combine data from multiple batches in a DStream
to generate results. To do that efficiently, they maintain a state or "remember" previously seen data that can
be combined with most current data.</p></div>
<div class="paragraph"><p>The main stateful transformation we will discuss is <em>updateStateByKey</em>.
Window transformations will be introduced
in a separate presentation.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformation-em-updatestatebykey-em">
<h2>Transformation <em>updateStateByKey</em></h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>updateStateByKey</strong>(<em>updateF</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new "state" DStream is formed by
applying a state update function <em>updateF</em> on the previous state of the key and
the new values of each key in the source DStream of Pair RDDs. The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>def updateF( newValues: Seq[stateType], oldState: Option[stateType]) : Option[stateType]
{
  //Compute and return a new state for a key
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>updateStateByKey</em> is used to maintain arbitrary state data for each key,
while continuously updating it with new information from incoming batches.</p></div>
<div class="paragraph"><p><em>stateType</em> is an arbitrary data type.</p></div>
<div class="paragraph"><p><em>updateF</em> is a function that, for a given key,
updates the state using the previous state and the new values from input stream for this key</p></div>
</div>
</div>
</section>
<section class="slide" id="important-requirement-for-em-updatestatebykey-em">
<h2>Important Requirement for <em>updateStateByKey</em></h2>
<div class="paragraph"><p><strong>Checkpointing must be enabled</strong></p></div>
<div class="ulist">
<ul>
<li>Checkpointing is a fault-tolerance mechanism in Spark Streaming</li>
<li>Checkpointing requires Spark to periodically save metadata and data into CFS</li>
<li>Configuring a checkpoint directory</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

// ...

ssc.checkpoint("checkpoint_directory")

ssc.start()
ssc.awaitTermination()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpointing is a fault-tolerance mechanism in Spark Streaming to avoid excessive re-computation of
previously processed data due to a failure.
Checkpointing saves RDDs generated by some stateful transformations that, otherwise,
would have to be recomputed through a very long lineage chain.</p></div>
<div class="paragraph"><p>Besides <em>updateStateByKey</em>, many but not all window transformations require mandatory checkpointing.
Window transformations are discussed in a separate presentation.</p></div>
<div class="paragraph"><p>Specifying a checkpoint directory enables checkpointing. A single statement is required before starting computation.</p></div>
</div>
</div>
</section>
<section class="slide" id="running-count-of-distinct-elements-in-a-dstream">
<h2>Running Count of Distinct Elements in a DStream</h2>
<div class="paragraph"><p><strong>Maintaining a state for each movie across stream batches</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="running count" src="images/spark/streaming/dstream-stateful-transformations/running-count.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate transformation <em>updateStateByKey</em>, let us count how many times each movie identifier (UUID) was seen up to the current moment.
To process the most current batch, we have to have counts from all previous batches (aka state).</p></div>
<div class="paragraph"><p>Transformation <em>countByValue</em> is stateless. It simply counts movies in a current batch and returns a DStream
of Pair RDDs.</p></div>
<div class="paragraph"><p>Transformation <em>updateStateByKey</em> is stateful. For each movie, it adds a count from the current batch and
a count from the previous state to generate a new state. For example, 10 + 4 = 14 as shown in the figure.
<em>updateStateByKey</em> generates a "state" DStream.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Defining the state update function and computation</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>def updateMovieCount(newValues: Seq[Long], oldCount: Option[Long])
: Option[Long] =
{
   if (newValues.isEmpty) Some(oldCount.getOrElse(0L))
   else Some(oldCount.getOrElse(0L) + newValues(0))
}</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)

stream.countByValue()
      .updateStateByKey[Long](updateMovieCount _)
      .print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>updateMovieCount</em> is executed on each individual movie UUID that is known to exist in a state or appears in
a current batch. It takes new values (in our example, we can have at most one value due to pre-aggregation with <em>countByValue</em>)
for the movie-key from the current batch and combines them with the old state-count.
It returns a new state-count for the movie-key.</p></div>
<div class="paragraph"><p><em>countByKey</em> generates a DStream of key-value pairs, where <em>key</em> is a movie UUID and <em>value</em> is how many times
this UUID appeared in a current input batch.</p></div>
<div class="paragraph"><p><em>updateStateByKey</em> computes a new state from a new input batch and a previous state. The result is a "state" DStream.</p></div>
<div class="paragraph"><p><em>print</em> outputs first 10 elements of each batch to the screen.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Sample output</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>-------------------------------------------
Time: 1443846676000 ms
-------------------------------------------
(3391c072-af52-4d3d-b909-ea6dfdfe475b,467)
(f7b1982a-3614-4ada-b5f1-a63291d10ad8,470)
(81074a01-602d-4d4c-a57b-8ff713042478,479)
(349cc28f-2a20-4e4b-862a-a4ce9f2916e2,474)
(f8729686-a166-48b5-9609-698592482b60,460)
(87032a77-0ccd-4161-ae69-44d70ce4c7da,465)
(6284b27a-a4ff-4118-b006-6327d438a1aa,433)
(f2c8c1f4-7c91-4644-bfc4-fa578957ff2d,479)
(e18a3eb1-a4aa-497b-8427-39677b4e92c7,498)
(d3c8af3f-206b-45e9-a2bd-38eaeb05e389,435)
...

-------------------------------------------
Time: 1443846680000 ms
-------------------------------------------
(3391c072-af52-4d3d-b909-ea6dfdfe475b,469)
(f7b1982a-3614-4ada-b5f1-a63291d10ad8,471)
(81074a01-602d-4d4c-a57b-8ff713042478,483)
(349cc28f-2a20-4e4b-862a-a4ce9f2916e2,483)
(f8729686-a166-48b5-9609-698592482b60,464)
(87032a77-0ccd-4161-ae69-44d70ce4c7da,469)
(6284b27a-a4ff-4118-b006-6327d438a1aa,436)
(f2c8c1f4-7c91-4644-bfc4-fa578957ff2d,481)
(e18a3eb1-a4aa-497b-8427-39677b4e92c7,505)
(d3c8af3f-206b-45e9-a2bd-38eaeb05e389,440)
...</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Results from two batches in a "state" DStream.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-window-transformations">
<h2>Operations on DStreams</h2>
<div class="paragraph"><p><strong>Two types of DStream operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li>Stateless transformations</li>
<li><p>
Stateful transformations<div class="ulist">
<ul>
<li>Transformation <em>updateStateByKey</em></li>
<li><strong>Window transformations</strong></li>
</ul>
</div></p></li>
</ul>
</div></p></li>
<li><em>Output operations</em></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on DStreams&#8201;&#8212;&#8201;transformations and output operations.</p></div>
<div class="paragraph"><p>Transformations transform input DStreams into new DStreams. They are evaluated lazily.
There are two types of DStream transformations: stateless and stateful.</p></div>
<div class="paragraph"><p>Stateful transformations include <em>updateStateByKey</em> and window transformations.</p></div>
<div class="paragraph"><p>Output operations trigger computation. Their results are printed to the screen or
written to a stable storage.</p></div>
<div class="paragraph"><p>This presentation focuses on window transformations.</p></div>
</div>
</div>
</section>
<section class="slide" id="how-do-window-transformations-work">
<h2>How Do Window Transformations Work?</h2>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 1" src="images/spark/streaming/dstream-window-transformations/window-transformation-1.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Window transformations combine data from all windowed batches in a DStream
to generate a new batch.</p></div>
<div class="paragraph"><p>A window is defined through these characteristics:</p></div>
<div class="ulist">
<ul>
<li><p>
Window has length or duration<div class="ulist">
<ul>
<li>Specified in time units&#8201;&#8212;&#8201;must be a multiple of a DStream batch interval</li>
<li><em>windowLength</em> = <em>n</em> x <em>batchInterval</em></li>
<li>Defines how many batches are included in windowed transformation</li>
<li>Example in the slide: <em>windowLength</em> = <em>2</em> x <em>batchInterval</em></li>
</ul>
</div></p></li>
<li><p>
Window has sliding interval or sliding duration<div class="ulist">
<ul>
<li>Specified in time units&#8201;&#8212;&#8201;must be a multiple of a DStream batch interval</li>
<li><em>slideInterval</em> = <em>m</em> x <em>batchInterval</em></li>
<li>Defines by how many batches to slide a window or how frequently to compute a windowed transformation</li>
<li>Example in the slide: <em>slideInterval</em> = <em>1</em> x <em>batchInterval</em></li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 2" src="images/spark/streaming/dstream-window-transformations/window-transformation-2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 3" src="images/spark/streaming/dstream-window-transformations/window-transformation-3.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 4" src="images/spark/streaming/dstream-window-transformations/window-transformation-4.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Window transformation computes a single RDD out of all windowed RDDs</strong></p></div>
<div class="ulist">
<ul>
<li>Window length or duration: <em>windowLength</em> = <em>n</em> x <em>batchInterval</em> (e.g., <em>n</em> = <em>2</em>)</li>
<li>Window sliding interval or sliding duration: <em>slideInterval</em> = <em>m</em> x <em>batchInterval</em> (e.g., <em>m</em> = <em>1</em>)</li>
</ul>
</div>
<div class="imageblock center">
<div class="content">
<img alt="window transformation 5" src="images/spark/streaming/dstream-window-transformations/window-transformation-5.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">

</div>
</div>
</section>
<section class="slide" id="choosing-a-window-length-and-a-sliding-interval">
<h2>Choosing a Window Length and a Sliding Interval</h2>
<div class="paragraph"><p><strong>General insights</strong></p></div>
<div class="ulist">
<ul>
<li><p>
Window length<div class="ulist">
<ul>
<li>Minute-scale window length is reasonable</li>
<li>Hour-scale window length is not recommended</li>
</ul>
</div></p></li>
<li><p>
Window sliding interval<div class="ulist">
<ul>
<li>Should meet application requirements</li>
<li>Should meet performance requirements</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li><p>
Window length<div class="ulist">
<ul>
<li>Minute-scale window length is reasonable</li>
<li><p>
Hour-scale window length is not recommended<div class="ulist">
<ul>
<li>Takes longer to process</li>
<li>Requires large batch intervals for stable processing</li>
<li>Requires a lot of lineage to recover from a failure</li>
<li>Alternative Solution: aggregate data in Cassandra instead</li>
</ul>
</div></p></li>
</ul>
</div></p></li>
<li><p>
Window sliding interval<div class="ulist">
<ul>
<li>Should meet application requirements</li>
<li>Should meet performance requirements (smaller sliding intervals require more computation)</li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="transformations-on-dstreams-of-generic-rdds-2">
<h2>Transformations on DStreams of Generic RDDs</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>window</strong>( <em>windowLength</em>, [<em>slideInterval</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by combining all windowed batches of the source DStream into
a single batch in a transformed DStream. The default <em>slideInterval</em> is the batch interval of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>countByWindow</strong>( <em>windowLength</em>, <em>slideInterval</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs with a single element representing a number of elements
in all windowed batches of the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>countByValueAndWindow</strong>( <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs containing a count for each distinct element
in all windowed batches of the source DStream. The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByWindow</strong>( <em>f</em>, <em>windowLength</em>, <em>slideInterval</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by RDDs with a single element generated by aggregating elements
in all windowed batches of the source DStream using an associative reduce function <em>f</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByWindow</strong>( <em>f</em>, <em>invF</em>, <em>windowLength</em>, <em>slideInterval</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A more efficient version of <em>reduceByWindow</em> with an inverse reduce function <em>invF</em> that allows incremental reduction.
While <em>f</em> is used to reduce new values entering a window, <em>invF</em> is used to "inverse reduce" old values leaving the same window.
This transformation is applicable when a reduce function is invertible (e.g., + is invertible with –).</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>window</em> is the simplest and most generic window transformation that can be used to implement
other window transformations. It simply creates a new DStream with batches whose intervals equal to a window length.</p></div>
<div class="paragraph"><p><em>countByWindow</em> is used to count elements falling into a window.</p></div>
<div class="paragraph"><p><em>countByValueAndWindow</em> is used to find how many times a value appears in a window.</p></div>
<div class="paragraph"><p><em>reduceByWindow</em> is for aggregating all elements in a window using a reduce function. Two versions of this
transformation exist: one with a reduce function only and one with both reduce and its  inverse function.
While the incremental version is, in general, more efficient,
it is only applicable when a reduce function has an inverse function.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-counting-elements-in-a-window">
<h2>Example: Counting Elements in a Window</h2>
<div class="paragraph"><p><strong>Three alternative solutions</strong></p></div>
<div class="listingblock">
<div class="title"><em>Using countByWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.countByWindow(Seconds(60), Seconds(4)).print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Using reduceByWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.map(m =&gt; 1).reduceByWindow({(a,b) =&gt; a+b}, Seconds(60), Seconds(4)).print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Using incremental reduceByWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.map(m =&gt; 1).reduceByWindow({(a,b) =&gt; a+b}, {(a,b) =&gt; a-b}, Seconds(60), Seconds(4)).print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Sample output</em></div>
<div class="content">
<pre class="CodeRay"><code>-------------------------------------------
Time: 1444015700000 ms
-------------------------------------------
2921

-------------------------------------------
Time: 1444015704000 ms
-------------------------------------------
2892

-------------------------------------------
Time: 1444015708000 ms
-------------------------------------------
2974</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Solving this simple problem to demonstrate three window transformations on DStreams
of generic RDDs.</p></div>
<div class="paragraph"><p>Notice the 60-second <em>windowLength</em> and 4-second <em>slideInterval</em>. The <em>batchInterval</em> is also 4 seconds (not shown in the slide).</p></div>
<div class="paragraph"><p>Also, notice that the total count is not necessarily increasing. When a window slides, some batches are added and some
batches are removed from a window, so the count may vary.</p></div>
</div>
</div>
</section>
<section class="slide" id="how-do-incremental-window-transformations-work">
<h2>How Do Incremental Window Transformations Work?</h2>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 1" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-1.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This example uses transformation <em>reduceByWindow</em> with reduce function <strong>+</strong> and inverse
reduce function <strong>-</strong> to compute a sum of all elements in windowed batches. The incremental transformation
relies on a "state" that remembers previously computed results.</p></div>
<div class="paragraph"><p>Let us see how the reduce function, its inverse, and "state" are used.</p></div>
<div class="paragraph"><p>Keep in mind that for this simple illustration, using an incremental transformation will not be more
beneficial than using a non-incremental transformations. The incremental effect becomes noticeable
for transformations with lengthy windows with short slide intervals.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 2" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Summation for a batch that entered the window PLUS previous window result/state MINUS summation for a batch that left the window.</p></div>
<div class="paragraph"><p>Compute 1+1+1 (from RDD1) = 3.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 3" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-3.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Summation for a batch that entered the window PLUS previous window result/state MINUS summation for a batch that left the window.</p></div>
<div class="paragraph"><p>Compute 1+1+1 (from RDD2) + 3 (previous state) = 6.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 4" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-4.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Summation for a batch that entered the window PLUS previous window result/state MINUS summation for a batch that left the window.</p></div>
<div class="paragraph"><p>Compute 1+1+1 (from RDD3) + 6 (previous state) -1-1-1 (from RDD1) = 6.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>reduceByWindow({(a,b) &#8658; a+b}, {(a,b) &#8658; a-b}, 2 x batchInterval, batchInterval)</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="incremental transformation 5" src="images/spark/streaming/dstream-window-transformations/incremental-transformation-5.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Summation for a batch that entered the window PLUS previous window result/state MINUS summation for a batch that left the window.</p></div>
<div class="paragraph"><p>Compute 1+1+1+1 (from RDD4) + 6 (previous state) -1-1-1 (from RDD2) = 7.</p></div>
</div>
</div>
</section>
<section class="slide" id="transformations-on-dstreams-of-pair-rdds-2">
<h2>Transformations on DStreams of Pair RDDs</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByKeyAndWindow</strong>( <em>f</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by aggregating values with the same key
in all windowed batches of the source DStream using an associative reduce function <em>f</em>.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>reduceByKeyAndWindow</strong>( <em>f</em>, <em>invF</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">An incremental version of <em>reduceByKeyAndWindow</em> with an inverse reduce function.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>groupByKeyAndWindow</strong>( <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A new DStream is formed by grouping values with the same key
in all windowed batches of the source DStream.
The optional
<em>numTasks</em> parameter specifies the number of reduce tasks to use in computation.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>These transformations on DStreams of Key-Value Pair RDDs would be conceptually equivalent to applying
<em>reduceByKey</em> and <em>groupByKey</em>, respectively, on each window in a DStream.</p></div>
<div class="paragraph"><p>They generate new DStreams of Key-Value Pair RDDs.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-counting-elements-per-key-in-a-window">
<h2>Example: Counting Elements per Key in a Window</h2>
<div class="paragraph"><p><strong>Two alternative solutions</strong></p></div>
<div class="listingblock">
<div class="title"><em>Using reduceByKeyAndWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.countByValue()
      .reduceByKeyAndWindow({(a:Long,b:Long) =&gt; a+b}, Seconds(60), Seconds(4))
      .print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Using incremental reduceByKeyAndWindow</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.countByValue()
      .reduceByKeyAndWindow({(a,b) =&gt; a+b}, {(a,b) =&gt; a-b}, Seconds(60), Seconds(4))
      .print()</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Sample output</em></div>
<div class="content">
<pre class="CodeRay"><code>-------------------------------------------
Time: 1444021448000 ms
-------------------------------------------
(3391c072-af52-4d3d-b909-ea6dfdfe475b,94)
(f7b1982a-3614-4ada-b5f1-a63291d10ad8,71)
(81074a01-602d-4d4c-a57b-8ff713042478,60)
(349cc28f-2a20-4e4b-862a-a4ce9f2916e2,78)
(f8729686-a166-48b5-9609-698592482b60,62)
(87032a77-0ccd-4161-ae69-44d70ce4c7da,85)
(6284b27a-a4ff-4118-b006-6327d438a1aa,73)
(f2c8c1f4-7c91-4644-bfc4-fa578957ff2d,73)
(e18a3eb1-a4aa-497b-8427-39677b4e92c7,62)
(d3c8af3f-206b-45e9-a2bd-38eaeb05e389,64)
...

-------------------------------------------
Time: 1444021452000 ms
-------------------------------------------
(3391c072-af52-4d3d-b909-ea6dfdfe475b,98)
(f7b1982a-3614-4ada-b5f1-a63291d10ad8,78)
(81074a01-602d-4d4c-a57b-8ff713042478,66)
(349cc28f-2a20-4e4b-862a-a4ce9f2916e2,82)
(f8729686-a166-48b5-9609-698592482b60,67)
(87032a77-0ccd-4161-ae69-44d70ce4c7da,88)
(6284b27a-a4ff-4118-b006-6327d438a1aa,75)
(f2c8c1f4-7c91-4644-bfc4-fa578957ff2d,77)
(e18a3eb1-a4aa-497b-8427-39677b4e92c7,66)
(d3c8af3f-206b-45e9-a2bd-38eaeb05e389,69)
...</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Solving this simple problem to demonstrate two window transformations on DStreams
of Pair RDDs.</p></div>
<div class="paragraph"><p>Notice the 60-second <em>windowLength</em> and 4-second <em>slideInterval</em>. The <em>batchInterval</em> is also 4 seconds (not shown in the slide).</p></div>
</div>
</div>
</section>
<section class="slide" id="important-checkpointing-requirements">
<h2>Important Checkpointing Requirements</h2>
<div class="paragraph"><p><strong>Checkpointing is mandatory for <em>countByWindow</em>, <em>countByValueAndWindow</em>, and incremental versions of <em>reduceByWindow</em> and <em>reduceByKeyAndWindow</em></strong></p></div>
<div class="ulist">
<ul>
<li>Checkpointing is a fault-tolerance mechanism in Spark Streaming</li>
<li>Checkpointing requires Spark to periodically save metadata and data into CFS</li>
<li>Configuring a checkpoint directory</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

// ...

ssc.checkpoint("checkpoint_directory")

ssc.start()
ssc.awaitTermination()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpointing is a fault-tolerance mechanism in Spark Streaming to avoid excessive re-computation of
previously processed data due to a failure.
Checkpointing saves RDDs generated by some stateful transformations that, otherwise,
would have to be recomputed through a very long lineage chain.</p></div>
<div class="paragraph"><p>Checkpointing is mandatory for many window transformations (see the slide), as well as
stateful transformation <em>updateStateByKey</em> (discussed in a separate presentation).</p></div>
<div class="paragraph"><p>Enabling checkpointing has a computational overhead and results in slower stream processing performance. However
checkpointing makes failure recovery much more efficient. It is all about trade-offs and
requirements of your specific use case.</p></div>
<div class="paragraph"><p>Specifying a checkpoint directory enables checkpointing. A single statement is required before starting computation.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-output-operations">
<h2>Operations on DStreams</h2>
<div class="paragraph"><p><strong>Two types of DStream operations</strong></p></div>
<div class="ulist">
<ul>
<li><p>
<em>Transformations</em><div class="ulist">
<ul>
<li>Stateless transformations</li>
<li>Stateful transformations</li>
</ul>
</div></p></li>
<li><p>
<em>Output operations</em><div class="ulist">
<ul>
<li><strong>Spark output operations</strong></li>
<li><strong>Spark-Cassandra Connector output operations</strong></li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark defines two types of operations on DStreams&#8201;&#8212;&#8201;transformations and output operations.</p></div>
<div class="paragraph"><p>Transformations transform input DStreams into new DStreams. They are evaluated lazily.
There are two types of DStream transformations: stateless and stateful.</p></div>
<div class="paragraph"><p>Output operations trigger computation. Their results are printed to the screen or
written to a stable storage. We can distinguish between output operations provided by Spark and
Spark-Cassandra Connector.</p></div>
<div class="paragraph"><p>This presentation focuses on output operations.</p></div>
</div>
</div>
</section>
<section class="slide" id="common-output-operations">
<h2>Common Output Operations</h2>
<div class="paragraph"><p><strong>Spark and Spark-Cassandra Connector output operations</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:40%" />
<col style="width:60%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Output Operation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>print</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Prints the first 10 elements of each RDD generated in the source DStream.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>saveAsTextFiles</strong>(<em>prefix</em>,[<em>suffix</em>])
<strong>saveAsObjectFiles</strong>(<em>prefix</em>,[<em>suffix</em>])
<strong>saveAsHadoopFiles</strong>(<em>prefix</em>,[<em>suffix</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Saves each RDD in the source DStream as a text file, Sequence file, or Hadoop file, respectively.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>foreachRDD</strong>(<em>f</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Applies a function <em>f</em> to each RDD in the source DStream and pushes data to an external system.
The function can use any transformations or actions on its input RDDs.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>saveToCassandra</strong>(<em>keyspace</em>,<em>table</em>,[<em>columns</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Saves each RDD in the source DStream into a Cassandra table. This
output operation is defined in <em>Spark-Cassandra Connector</em>.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are some frequently used output operations that are evaluated in parallel on the source DStream
and whose results are either printed or pushed to a file or a database.</p></div>
<div class="paragraph"><p>You might have seen <em>print</em> before&#8201;&#8212;&#8201;it simply outputs the first 10 elements of each RDD in the source DStream.</p></div>
<div class="paragraph"><p>We will demonstrate <em>saveToCassandra</em> and <em>foreachRDD</em> in the following examples.</p></div>
</div>
</div>
</section>
<section class="slide" id="saving-a-raw-data-stream-into-cassandra">
<h2>Saving a Raw Data Stream into Cassandra</h2>
<div class="paragraph"><p><strong>Storying individual movie clicks</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="saving raw" src="images/spark/streaming/dstream-output-operations/saving-raw.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Let us solve a useful problem of storying a raw stream of movie clicks into a Cassandra table.</p></div>
<div class="paragraph"><p>Given an input stream of movie identifiers (UUIDs), we need to generate a <em>click_id</em> (TIMEUUID) for each movie identifier
and save all data into table <em>clicks_by_movie</em>.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>STEP 1: Creating a table</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val cc = com.datastax.spark.connector.cql.CassandraConnector(conf)

cc.withSessionDo { session =&gt;
  session.execute("CREATE TABLE IF NOT EXISTS " +
                  "killr_video.clicks_by_movie ( " +
                  "movie_id UUID, " +
                  "click_id TIMEUUID, " +
                  "PRIMARY KEY(movie_id,click_id));")
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To create the <em>clicks_by_movie</em> table in our streaming application, we use <em>CassandraConnector</em>
that allows executing CQL statements like CREATE TABLE in this example.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>STEP 2: Creating, augmenting and saving a data stream</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)

stream.map(m =&gt; (java.util.UUID.fromString(m),
                 com.datastax.driver.core.utils.UUIDs.timeBased()))
      .saveToCassandra("killr_video","clicks_by_movie",
                       SomeColumns("movie_id","click_id"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Notice how we deal with UUIDs and TIMEUUIDs in transformation <em>map</em>.</p></div>
<div class="paragraph"><p>Each RDD in the stream is stored into Cassandra using output operation <em>saveToCassandra</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="saving-an-aggregated-data-stream-into-cassandra">
<h2>Saving an Aggregated Data Stream into Cassandra</h2>
<div class="paragraph"><p><strong>Storying click aggregates per movie</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="saving aggregates" src="images/spark/streaming/dstream-output-operations/saving-aggregates.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Let us solve a useful problem of aggregating data in a stream of movie clicks and storying results
into a Cassandra table.</p></div>
<div class="paragraph"><p>Given an input stream of movie identifiers (UUIDs), we need to
pre-compute how many times each movie appears in a batch
and save the aggregates into table <em>clicks_per_movie</em>.</p></div>
<div class="paragraph"><p>Notice that <em>total_clicks</em> is a column of
type <em>COUNTER</em>&#8201;&#8212;&#8201;aggregates from different batches for each movie will be added up by Cassandra!</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>STEP 1: Creating a table</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val cc = com.datastax.spark.connector.cql.CassandraConnector(conf)

cc.withSessionDo { session =&gt;
  session.execute("CREATE TABLE IF NOT EXISTS " +
                  "killr_video.clicks_per_movie ( " +
                  "movie_id UUID, " +
                  "total_clicks COUNTER, " +
                  "PRIMARY KEY(movie_id));")
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To create the <em>clicks_per_movie</em> table in our streaming application, we use <em>CassandraConnector</em>
that allows executing CQL statements like CREATE TABLE in this example.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>STEP 2: Creating, pre-aggregating and saving a data stream</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)

stream.map(id =&gt; (java.util.UUID.fromString(id),1))
      .reduceByKey(_ + _)
      .saveToCassandra("killr_video","clicks_per_movie",
                       SomeColumns("movie_id","total_clicks"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p><em>map</em> is used to transform a stream of generic RDDs to a stream of key-value Pair RDDs.</p></div>
<div class="paragraph"><p>Pre-aggregation for each batch is done in transformation <em>reduceByKey</em>.</p></div>
<div class="paragraph"><p>Each RDD in the stream is stored into Cassandra using output operation <em>saveToCassandra</em>. Cassandra completes
the final stage of aggregation using counters.</p></div>
</div>
</div>
</section>
<section class="slide" id="the-role-of-em-foreachrdd-em-output-operation">
<h2>The Role of <em>foreachRDD</em> Output Operation</h2>
<div class="paragraph"><p><strong>Most generic output operation</strong></p></div>
<div class="listingblock">
<div class="title"><em>Saving an Aggregated Data Stream into Cassandra</em></div>
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)

stream.foreachRDD(rdd =&gt;
       rdd.map(id =&gt; (java.util.UUID.fromString(id),1))
          .reduceByKey(_ + _)
          .saveToCassandra("killr_video","clicks_per_movie",
                           SomeColumns("movie_id","total_clicks"))
       )</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="ulist">
<ul>
<li><p>
<em>foreachRDD</em> is the most generic output operation that allows
using any Spark Core transformations and actions on RDDs inside of DStream.<div class="ulist">
<ul>
<li>Not all Spark Core operations on RDDs have equivalent Spark Streaming operations on DStreams -–
this is when <em>foreachRDD</em> is the solution!</li>
<li><em>foreachRDD</em> can be used to define custom computation on RDDs.</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="paragraph"><p>The code solves the previous problem using <em>foreachRDD</em>. Note that, this time, <em>map</em>, <em>reduceByKey</em>, and
<em>saveToCassandra</em> are RDD operations and not DStream operations like in the previous example.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-checkpointing-and-recovery">
<h2>Checkpointing</h2>
<div class="paragraph"><p><strong>Important fault-tolerance mechanism in Spark Streaming</strong></p></div>
<div class="ulist">
<ul>
<li>Requires Spark to periodically save metadata and data into CFS</li>
<li>Limits the state that needs to be recomputed on failure</li>
<li>Enables application recovery from a checkpoint</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpointing is a fault-tolerance mechanism in Spark Streaming to avoid excessive re-computation of
previously processed data due to a failure.</p></div>
<div class="ulist">
<ul>
<li><p>
Metadata<div class="ulist">
<ul>
<li>Streaming application configuration</li>
<li>DStream operations that define a streaming application</li>
<li>Incomplete batches of data</li>
</ul>
</div></p></li>
<li><p>
Data<div class="ulist">
<ul>
<li>DStream RDDs generated by transformations</li>
</ul>
</div></p></li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="transformations-that-require-checkpointing">
<h2>Transformations that Require Checkpointing</h2>
<div class="paragraph"><p><strong>Mandatory checkpointing</strong></p></div>
<div class="ulist">
<ul>
<li>Stateful transformation <em>updateStateByKey</em></li>
<li><p>
Many window transformations<div class="ulist">
<ul>
<li><em>countByWindow</em></li>
<li><em>countByValueAndWindow</em></li>
<li>incremental <em>reduceByWindow</em></li>
<li>incremental <em>reduceByKeyAndWindow</em></li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpoining is mandatory for a number of window transformations (see the slide), as well as
stateful transformation <em>updateStateByKey</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="enabling-checkpointing">
<h2>Enabling Checkpointing</h2>
<div class="paragraph"><p><strong>Setting a checkpointing directory</strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

// ...

ssc.checkpoint("checkpoint_directory")

ssc.start()
ssc.awaitTermination()</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpoing has a computational overhead and results in slower stream processing performance. However
checkpointing makes failure recovery much more efficient. It is all about trade-offs and
requirements of your specific use case.</p></div>
<div class="paragraph"><p>Checkpointing affects processing times. Consider not enabling it for applications that do not
require checkpointing</p></div>
<div class="ulist">
<ul>
<li>No stateful transformations</li>
<li>Driver fault tolerance is not critical</li>
<li>Some data loss is acceptable</li>
</ul>
</div>
<div class="paragraph"><p>Specifying a checkpoint directory enables checkpointing. A single statement is required before starting computation.</p></div>
<div class="paragraph"><p>In DSE, checkpoint directory is created in CFS by default.</p></div>
</div>
</div>
</section>
<section class="slide" id="configuring-a-checkpointing-interval">
<h2>Configuring a Checkpointing Interval</h2>
<div class="paragraph"><p><strong>Checkpointing interval can be adjusted for each DStream</strong></p></div>
<div class="ulist">
<ul>
<li>Default checkpointing interval is a multiple of the batch interval that is at least 10 seconds</li>
<li>Frequent checkpointing = slower performance, faster failure recovery</li>
<li>Infrequent checkpointing = faster performance, slower failure recovery</li>
<li>General recommendation: <em>5</em>-<em>10</em> x <em>slideInterval</em></li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>stream.checkpoint(Seconds(20))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Checkpointing is performed periodically. A checkpointing interval is customizable for each DStream.</p></div>
</div>
</div>
</section>
<section class="slide" id="recovering-from-a-checkpoint">
<h2>Recovering from a Checkpoint</h2>
<div class="paragraph"><p><strong>Function <em>main</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>def main(args: Array[String]) {
    val sparkMasterHost = args(0)
    val cassandraHost   = args(1)
    val streamHost      = args(2)
    val checkpointDir   = args(3)

    val ssc = StreamingContext.getOrCreate(checkpointDir,
              () =&gt; {createStreamingContext(sparkMasterHost,
                     cassandraHost, streamHost, checkpointDir)})

    ssc.start()
    ssc.awaitTermination()
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>When checkpointing is enabled, an application can take an advantage of recovering from a
checkpoint. However, to do so, the application should be designed a bit differently.</p></div>
<div class="paragraph"><p>Take a look at the <em>main</em> function. The most important part is how a <em>StreamingContext</em> is created
using method <em>getOrCreate</em>:</p></div>
<div class="ulist">
<ul>
<li>When the program is started for the first time, it creates a new <em>StreamingContext</em> using function <em>createStreamingContext</em> that we define in the next slide.</li>
<li>When the program is restarted after a failure, it recreates a <em>StreamingContext</em> from the checkpoint data in the checkpoint directory.</li>
</ul>
</div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Function <em>createStreamingContext</em></strong></p></div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>def createStreamingContext(
    sparkMasterHost: String, cassandraHost: String,
    streamHost: String, checkpointDir: String)
    : StreamingContext = {

    val conf = new SparkConf(true)
      .setAppName("Checkpointing Demo")
      .setMaster("spark://" + sparkMasterHost + ":7077")
      .set("spark.cassandra.connection.host", cassandraHost)
      .set("spark.cleaner.ttl", "3600")
      .setJars(Array(System.getProperty("user.dir") + "/target/scala-2.10/checkpointing-demo_2.10-0.1.jar"))

    val ssc = new StreamingContext(conf, Seconds(4))

    val stream = ssc.socketTextStream(streamHost, 9999)
    stream.checkpoint(Seconds(20))

    // Define computation

    ssc.checkpoint(checkpointDir)
    ssc
}</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This function is only called once, when an application is started for the first time.</p></div>
<div class="paragraph"><p>The code should be familiar.
It creates the <em>SparkConf</em> and <em>StreamingContext</em> objects, as well as sets up streams and computation.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-dstream-persistence">
<h2>The Challenge: Suboptimal Code</h2>
<div class="paragraph"><p><strong>Counting clicks per movie using Spark Streaming and Cassandra</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="clicks per movie" src="images/spark/streaming/dstream-persistence/clicks_per_movie.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)
                .map(id =&gt; (java.util.UUID.fromString(id),1))
                .reduceByKey(_ + _)

stream.saveToCassandra("killr_video","clicks_per_movie",
                 SomeColumns("movie_id","total_clicks"))

stream.print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Study this simple program that serves as our running example in this presentation.</p></div>
<div class="paragraph"><p>We are pre-aggregating counts per movie for each batch using Spark Streaming (transformation
<em>reduceByKey</em>) and calculating final counts using counters in Cassandra table <em>clicks_per_movie</em>.
We are also printing the first 10 partial aggregates to the screen.</p></div>
<div class="paragraph"><p>Notice that we are using two output operations (<em>saveToCassandra</em> and <em>print</em>), which requires recomputing results of transformations
<em>map</em> and <em>reduceByKey</em> twice. This is the same issue that we have seen with RDDs and multiple actions.</p></div>
<div class="paragraph"><p>This program is simple and it works. However, it is suboptimal and we will optimize this code to run faster!</p></div>
</div>
</div>
</section>
<section class="slide" id="dstream-persistence">
<h2>DStream Persistence</h2>
<div class="paragraph"><p><strong>Important optimization for DStreams used with multiple output operations</strong></p></div>
<div class="ulist">
<ul>
<li>You can instruct Spark Streaming to cache or persist RDDs in a DStream</li>
<li>Persisted DStream is kept serialized in memory (by default) once it is computed for the first time</li>
<li>Persisted DStream is reused by other operations without recomputing</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>The persistence mechanism should be used to avoid recomputing the same DStream multiple times.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The mechanism that allows us to materialize and reuse a DStream in Spark Streaming is called <em>DStream persistence</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="dstream-persistence-api">
<h2>DStream Persistence API</h2>
<div class="paragraph"><p><strong>Transformations <em>persist</em> and <em>cache</em></strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>persist</strong>([<em>storageLevel</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persists the source DStream according to a storage level specified by the optional parameter. The default storage level is
<em>org.apache.spark.storage.StorageLevel.MEMORY_ONLY_SER</em>, which prescribes persisting
elements of RDDs in the source DStream as serialized Java objects in the JVM.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cache</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Same as <em>persist</em>() or <em>persist</em>(<em>StorageLevel.MEMORY_ONLY_SER</em>).</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>When using <em>cache</em>() or <em>persist</em>(), if data does not fit into memory,
some partitions will not be cached and will be recomputed on the fly when needed.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are the two transformations of the DStream Persistence API.</p></div>
<div class="paragraph"><p>More storage levels for <em>persist</em> are discussed on the next slide.</p></div>
<div class="paragraph"><p><em>cache</em> is a convenient synonym of <em>persist</em> with the default storage level.</p></div>
<div class="paragraph"><p>Spark Streaming unpersists DStream RDDs automatically, as soon as partitions
are no longer needed for computation.</p></div>
<div class="paragraph"><p>The <strong>important</strong> difference between DStream Persistence API and RDD Persistence API
is that the former uses <em>MEMORY_ONLY_SER</em> as the default storage level, while the latter
uses <em>MEMORY_ONLY</em> as the default storage level.</p></div>
</div>
</div>
</section>
<section class="slide" id="storage-levels">
<h2>Storage Levels</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Storage Level</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_ONLY</em> <em>MEMORY_ONLY_SER</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting elements of RDDs in a DStreams as <em>deserialized</em> or <em>serialized</em> Java objects in the JVM, respectively. Partitions
that do not fit into memory are not cached and recomputed when needed.
<em>MEMORY_ONLY_SER</em> is more space-efficient but more CPU-intensive than <em>MEMORY_ONLY</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_AND_DISK</em> <em>MEMORY_AND_DISK_SER</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting elements of RDDs in a DStreams as <em>deserialized</em> or <em>serialized</em> Java objects in the JVM, respectively. Partitions
that do not fit into memory are spilled to disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>DISK_ONLY</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting a Dstream on disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_ONLY_2</em>, <em>MEMORY_AND_DISK_2</em>, &#8230;&#8203;</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Same as the respective storage levels above, but with replication on two nodes in a cluster.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>OFF_HEAP</em> (experimental)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting elements of RDDs in a DStreams in <em>serialized</em> format in <em>Tachyon</em> (a memory-centric distributed storage system).</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>In some cases, recomputing partitions may be faster than reading persisted partitions from disk!</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Study the different storage level possibilities for <em>persist</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="automatically-persisted-dstreams">
<h2>Automatically Persisted DStreams</h2>
<div class="paragraph"><p><strong>Explicit use of <em>persist</em> or <em>cache</em> is not required</strong></p></div>
<div class="ulist">
<ul>
<li><p>
Spark automatically persists DStreams generated by stateful transformations<div class="ulist">
<ul>
<li>Transformation <em>updateStateByKey</em></li>
<li>Window transformations</li>
</ul>
</div></p></li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Stateful transformations require recomputing the same batches in a DStream multiple times.
Therefore, Spark automatically persists DStreams generated by stateful transformations in memory.</p></div>
<div class="paragraph"><p>Explicit use of <em>persist</em> or <em>cache</em> is not required.</p></div>
</div>
</div>
</section>
<section class="slide" id="challenge-solution">
<h2>Challenge Solution</h2>
<div class="paragraph"><p><strong>Counting clicks per movie using Spark Streaming and Cassandra</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="clicks per movie" src="images/spark/streaming/dstream-persistence/clicks_per_movie.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val ssc = new StreamingContext(conf, Seconds(4))

val stream = ssc.socketTextStream(streamHost, 9999)
                .map(id =&gt; (java.util.UUID.fromString(id),1))
                .reduceByKey(_ + _)
                .cache

stream.saveToCassandra("killr_video","clicks_per_movie",
                 SomeColumns("movie_id","total_clicks"))

stream.print</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here is our challenge solution! We only added caching for the DStream
to make our code run faster.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-streaming-controlling-parallelism">
<h2>Controlling Parallelism</h2>
<div class="paragraph"><p><strong>Two types of tasks</strong></p></div>
<div class="ulist">
<ul>
<li>Receiver tasks = number of receivers</li>
<li>Data processing tasks = number of partitions</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>The "right" number of tasks is frequently found empirically.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>There are two types of tasks executed by <em>Executors</em> in Spark Streaming applications.</p></div>
<div class="paragraph"><p>We can control parallelism by controlling the number of receivers and the number of partitions.
There is always only one receiver per stream. The number of partitions can be controlled for
each batch/RDD in a DStream.</p></div>
</div>
</div>
</section>
<section class="slide" id="number-of-receivers">
<h2>Number of Receivers</h2>
<div class="paragraph"><p><strong>Creating multiple DStreams results in multiple receivers</strong></p></div>
<div class="ulist">
<ul>
<li>Receiving multiple streams from the same source</li>
<li>Having multiple data stream sources</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay"><code>val stream1 = ssc.socketTextStream(...)
val stream2 = KafkaUtils.createStream(...)
val stream3 = KafkaUtils.createStream(...)
val stream4 = stream2.union(stream3)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To increase the number of receiver tasks, you have to increase the number of streams in your program.</p></div>
<div class="paragraph"><p>In some cases, it is straightforward to do
if an input stream source can be used to generate multiple streams (e.g., subscribe to multiple topics in a publish/subscribe system).</p></div>
<div class="paragraph"><p>In other cases, additional work may be required,
such as "manually" splitting the source into multiple sources (e.g., data in different locations in CFS).</p></div>
<div class="paragraph"><p>This example has two stream sources: TCP socket and Kafka. There are two Kafka streams that read different topics
from the same source and are unified into a single stream with <em>union</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="number-of-partitions">
<h2>Number of Partitions</h2>
<div class="paragraph"><p><strong>Three mechanisms for controlling partitioning in a DStream</strong></p></div>
<div class="listingblock">
<div class="title"><em>Default parallelism</em></div>
<div class="content">
<pre class="CodeRay"><code>val conf = new SparkConf(true).set("spark.default.parallelism","10")</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Explicit repartitioning</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.repartition(2 * ssc.sparkContext.defaultParallelism)</code></pre>
</div>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="listingblock">
<div class="title"><em>Operation parameters</em></div>
<div class="content">
<pre class="CodeRay"><code>stream.countByValue(2 * ssc.sparkContext.defaultParallelism)</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>All three mechanisms prescribe how many partitions are created in each RDD of a DStream.
Optimal partitioning results in optimal performance.</p></div>
<div class="paragraph"><p>By controlling a number of partitions in RDDs of a DStream, we affect a number
of tasks that execute our operations and, ultimately, the level of parallelism for
each operation. If a task is running for too long and fails, re-executing the task will still
take long time. If tasks are running for a few milliseconds, then it is likely that scheduling time
will have a considerable impact on overall performance of an application. Finding a balance between
task execution and scheduling times is the first main reason to pay attention to partitioning.</p></div>
<div class="paragraph"><p>Note that there is only one repartitioning transformation for DStreams&#8201;&#8212;&#8201;<em>repartition</em>.
There are not equivalents of RDD&#8217;s <em>coalesce</em> and
<em>partitionBy</em> for DStreams.</p></div>
</div>
</div>
</section>
<div aria-role="navigation">
<a class="deck-prev-link" href="#" title="Previous">
<i class="icon-chevron-with-circle-left"></i>
</a>
<a class="deck-next-link" href="#" title="Next">
<i class="icon-chevron-with-circle-right"></i>
</a>
</div>
</div>
<script src="deck.js/jquery.min.js"></script>
<script src="deck.js/d3.v2.js"></script>
<script src="deck.js/jquery-ui.min.js"></script>
<script src="deck.js/core/deck.core.js"></script>
<script src="deck.js/extensions/scale/deck.scale.js"></script>
<script src="deck.js/extensions/navigation/deck.navigation.js"></script>
<script src="deck.js/extensions/split/deck.split.js"></script>
<script src="deck.js/extensions/animation/deck.animation.js"></script>
<script src="deck.js/extensions/deck.js-notes/deck.notes.js"></script>
<script src="deck.js/extensions/goto/deck.goto.js"></script>
<script src="deck.js/extensions/clone/deck.clone.js"></script>
<script src="deck.js/extensions/svg/svg.min.js"></script>
<script src="js/course.js"></script>
<footer>
<div class="flex-element deck-course">
<p>&copy; 2016 DataStax. Use only with permission. &bull;
<span class="course-title">Spark Streaming</span></p>
</div>
<div class="flex-element deck-brand">
<a href="http://academy.datastax.com" target="blank">DataStax Academy</a>
</div>
<div class="deck-progressbar">
<span></span>
</div>
</footer>
<script type="text/javascript">
  //<![CDATA[
    (function($, deck, undefined) {
      $.deck.defaults.keys['previous'] = [8, 33, 37, 39];
      $.deck.defaults.keys['next'] = [13, 32, 34, 39];
    
      $.extend(true, $[deck].defaults, {
          countNested: false
      });
    
      $.deck('.slide');
      $.deck('disableScale');
    })(jQuery, 'deck');
  //]]>
</script>
<script type="text/javascript">
  //<![CDATA[
    $(document).bind('deck.change', function(event, from, to) {
      var width = to / ($.deck('getSlides').length - 1) * 100;
      $('.deck-progressbar span').css('width', width + '%');
    });
  //]]>
</script>
</body>
</html>