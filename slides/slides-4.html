<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type" />
<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible" />
<meta content="Asciidoctor 1.5.2" name="generator" />
<title>Optimization is not Always Premature</title>
<link href="deck.js/themes/style/font.css" rel="stylesheet" />
<style>
.conum { display: inline-block; color: white !important; background-color: #222222; -webkit-border-radius: 100px; border-radius: 100px; text-align: center; width: 1.2em; height: 1.2em; font-size: 0.9em; font-weight: bold; line-height: 1.2; font-family: Arial, sans-serif; font-style: normal; position: relative; top: -0.1em; }
.conum * { color: white !important; }
.conum + b { display: none; }
.conum:after { content: attr(data-value); }
.conum:not([data-value]):empty { display: none; }
.colist table td:first-of-type { padding-right: 0.25em; }
</style>
<style>
/* Stylesheet for CodeRay to match GitHub theme | MIT License | http://foundation.zurb.com */
/*pre.CodeRay {background-color:#f7f7f8;}*/
.CodeRay .line-numbers{border-right:1px solid #d8d8d8;padding:0 0.5em 0 .25em}
.CodeRay span.line-numbers{display:inline-block;margin-right:.5em;color:rgba(0,0,0,.3)}
.CodeRay .line-numbers strong{font-weight: normal}
table.CodeRay{border-collapse:separate;border-spacing:0;margin-bottom:0;border:0;background:none}
table.CodeRay td{vertical-align: top}
table.CodeRay td.line-numbers{text-align:right}
table.CodeRay td.line-numbers>pre{padding:0;color:rgba(0,0,0,.3)}
table.CodeRay td.code{padding:0 0 0 .5em}
table.CodeRay td.code>pre{padding:0}
.CodeRay .debug{color:#fff !important;background:#000080 !important}
.CodeRay .annotation{color:#007}
.CodeRay .attribute-name{color:#000080}
.CodeRay .attribute-value{color:#700}
.CodeRay .binary{color:#509}
.CodeRay .comment{color:#998;font-style:italic}
.CodeRay .char{color:#04d}
.CodeRay .char .content{color:#04d}
.CodeRay .char .delimiter{color:#039}
.CodeRay .class{color:#458;font-weight:bold}
.CodeRay .complex{color:#a08}
.CodeRay .constant,.CodeRay .predefined-constant{color:#008080}
.CodeRay .color{color:#099}
.CodeRay .class-variable{color:#369}
.CodeRay .decorator{color:#b0b}
.CodeRay .definition{color:#099}
.CodeRay .delimiter{color:#000}
.CodeRay .doc{color:#970}
.CodeRay .doctype{color:#34b}
.CodeRay .doc-string{color:#d42}
.CodeRay .escape{color:#666}
.CodeRay .entity{color:#800}
.CodeRay .error{color:#808}
.CodeRay .exception{color:inherit}
.CodeRay .filename{color:#099}
.CodeRay .function{color:#900;font-weight:bold}
.CodeRay .global-variable{color:#008080}
.CodeRay .hex{color:#058}
.CodeRay .integer,.CodeRay .float{color:#099}
.CodeRay .include{color:#555}
.CodeRay .inline{color:#00}
.CodeRay .inline .inline{background:#ccc}
.CodeRay .inline .inline .inline{background:#bbb}
.CodeRay .inline .inline-delimiter{color:#d14}
.CodeRay .inline-delimiter{color:#d14}
.CodeRay .important{color:#555;font-weight:bold}
.CodeRay .interpreted{color:#b2b}
.CodeRay .instance-variable{color:#008080}
.CodeRay .label{color:#970}
.CodeRay .local-variable{color:#963}
.CodeRay .octal{color:#40e}
.CodeRay .predefined{color:#369}
.CodeRay .preprocessor{color:#579}
.CodeRay .pseudo-class{color:#555}
.CodeRay .directive{font-weight:bold}
.CodeRay .type{font-weight:bold}
.CodeRay .predefined-type{color:inherit}
.CodeRay .reserved,.CodeRay .keyword {color:#000;font-weight:bold}
.CodeRay .key{color:#808}
.CodeRay .key .delimiter{color:#606}
.CodeRay .key .char{color:#80f}
.CodeRay .value{color:#088}
.CodeRay .regexp .delimiter{color:#808}
.CodeRay .regexp .content{color:#808}
.CodeRay .regexp .modifier{color:#808}
.CodeRay .regexp .char{color:#d14}
.CodeRay .regexp .function{color:#404;font-weight:bold}
.CodeRay .string{color:#d20}
.CodeRay .string .string .string{background:#ffd0d0}
.CodeRay .string .content{color:#d14}
.CodeRay .string .char{color:#d14}
.CodeRay .string .delimiter{color:#d14}
.CodeRay .shell{color:#d14}
.CodeRay .shell .delimiter{color:#d14}
.CodeRay .symbol{color:#990073}
.CodeRay .symbol .content{color:#a60}
.CodeRay .symbol .delimiter{color:#630}
.CodeRay .tag{color:#008080}
.CodeRay .tag-special{color:#d70}
.CodeRay .variable{color:#036}
.CodeRay .insert{background:#afa}
.CodeRay .delete{background:#faa}
.CodeRay .change{color:#aaf;background:#007}
.CodeRay .head{color:#f8f;background:#505}
.CodeRay .insert .insert{color:#080}
.CodeRay .delete .delete{color:#800}
.CodeRay .change .change{color:#66f}
.CodeRay .head .head{color:#f4f}
</style>
<link href="deck.js/core/deck.core.css" rel="stylesheet" />
<link href="deck.js/extensions/scale/deck.scale.css" media="screen" rel="stylesheet" />
<link href="deck.js/themes/style/datastax.css" media="screen" rel="stylesheet" />
<link href="deck.js/themes/transition/fade.css" media="screen" rel="stylesheet" />
<link href="deck.js/core/print.css" media="print" rel="stylesheet" />
<script src="deck.js/modernizr.custom.js"></script>
</head>
<body class="article">
<div class="deck-container">
<section class="slide" id="title-slide">
<h1>Optimization is not Always Premature</h1>
</section>
<section class="slide" id="spark-optimization-techniques-broadcast-variables">
<h2>Shared Variables in Spark</h2>
<div class="paragraph"><p><strong>Used to optimize your code performance</strong></p></div>
<div class="ulist">
<ul>
<li>Broadcast variables</li>
<li>Accumulator variables</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>Shared variables are purely for optimization and efficiency.</p></div>
<div class="paragraph"><p>You only need to use shared variables explicitly when a use case is right.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark supports two types of shared variables: broadcast variables and accumulator variables.
You only need to use shared variables explicitly to optimize your code in certain situations.</p></div>
<div class="paragraph"><p>In this presentation, we will discuss broadcast variables.</p></div>
</div>
</div>
</section>
<section class="slide" id="broadcast-variables">
<h2>Broadcast Variables</h2>
<div class="paragraph"><p><strong>Definition and use cases</strong></p></div>
<div class="paragraph"><p><em><strong>Definition</strong></em></p></div>
<div class="verseblock">
<pre class="content">Broadcast variables are read-only variables whose values are
broadcasted and cached on each node in a cluster and are
visible to all tasks of your application</pre>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="paragraph"><p><em><strong>Main use cases</strong></em></p></div>
<div class="ulist">
<ul>
<li>Application tasks across multiple stages need the same, relatively large and immutable dataset</li>
<li>Application tasks need the same, relatively large and immutable dataset cached in deserialized form</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Broadcast variables are read-only variables whose values are
broadcasted and cached on each node in a cluster and are
visible to all tasks of your application.</p></div>
<div class="paragraph"><p>Spark uses special algorithms that reduce communication costs when broadcasting data.
However, if a broadcast variable is used in only one task on each node in a cluster,
the benefits are negligible in comparison with a regular variable.</p></div>
<div class="paragraph"><p>The main reasons to use broadcast variables include:</p></div>
<div class="ulist">
<ul>
<li>Application tasks across multiple stages need the same, relatively large and immutable dataset</li>
<li>Application tasks need the same, relatively large and immutable dataset cached in deserialized form</li>
</ul>
</div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Statement</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">val <em>broadcastVar</em> = <em>sc</em>.<em>broadcast</em>(<em>object</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Declare and initialize broadcast variable <em>broadcastVar</em> with the <em>object</em> value of any type
using method <em>broadcast</em> of the Spark Context object <em>sc</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>broadcastVar</em>.<em>value</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Access <em>value</em> of broadcast variable <em>broadcastVar</em> in a Spark operation, such as <em>map</em>, <em>filter</em>, or <em>reduce</em>.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Broadcast variables are very simple to create and use.</p></div>
</div>
</div>
</section>
<section class="slide" id="example">
<h2>Example</h2>
<div class="paragraph"><p><strong>Not using broadcast variables</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/optimization-techniques/broadcast-variables/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val popularTitles = Set("Alice in Wonderland",
                        "Alice Through the Looking Glass", "...")

val movies = sc.cassandraTable("killr_video","movies")
               .select("title","release_year","rating","genres")
               .cache

movies.filter(row =&gt; popularTitles contains row.getString("title"))
      .saveToCassandra("killr_video", "favorite_movies",
                       SomeColumns("title","release_year","rating","genres"))

movies.filter(row =&gt; !(popularTitles contains row.getString("title")))
      .saveToCassandra("killr_video", "other_movies",
                       SomeColumns("title","release_year","rating","genres"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate when and how to use broadcast variables, let us explore this example first. We are not using
broadcast variables yet.</p></div>
<div class="paragraph"><p>Here, we have a regular application variable <em>popularTitles</em> (technically, it is a value) that holds
a set of movie titles. We also retrieve data from Cassandra table <em>movies</em> into RDD <em>movies</em> and cache this RDD
because it is used with multiple actions.
The next statement filters RDD <em>movies</em> and saves only those Cassandra rows into table <em>favorite_movies</em> that
have popular titles. The last statement filters RDD <em>movies</em> and saves only those Cassandra rows into
table <em>other_movies</em> that do not have popular titles.</p></div>
<div class="paragraph"><p>The computation logic is quite simple and this code runs with no errors. However, notice
that <em>popularTitles</em> is used in two <em>filter</em> transformations and therefore, its value will be copied to two
respective tasks on each node executing these transformations. If <em>popularTitles</em> is a large dataset,
this is when we should consider using
a broadcast variable to hold popular titles to optimize our code.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Using broadcast variables</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/optimization-techniques/broadcast-variables/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val popularTitles = sc.broadcast(Set("Alice in Wonderland",
                                     "Alice Through the Looking Glass", "..."))

val movies = sc.cassandraTable("killr_video","movies")
               .select("title","release_year","rating","genres")
               .cache

movies.filter(row =&gt; popularTitles.value contains row.getString("title"))
      .saveToCassandra("killr_video", "favorite_movies",
                       SomeColumns("title","release_year","rating","genres"))

movies.filter(row =&gt; !(popularTitles.value contains row.getString("title")))
      .saveToCassandra("killr_video", "other_movies",
                       SomeColumns("title","release_year","rating","genres"))</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This code is now optimized!
<em>popularTitle</em> is a broadcast variable whose value is copied and cached on each node only one time.
The value of <em>popularTitles</em> is then shared by both <em>filter</em> transformation tasks running on each node.</p></div>
</div>
</div>
</section>
<section class="slide" id="spark-optimization-techniques-accumulator-variables">
<h2>Shared Variables in Spark</h2>
<div class="paragraph"><p><strong>Used to optimize your code performance</strong></p></div>
<div class="ulist">
<ul>
<li>Broadcast variables</li>
<li>Accumulator variables</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>Shared variables are purely for optimization and efficiency.</p></div>
<div class="paragraph"><p>You only need to use shared variables explicitly when a use case is right.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark supports two types of shared variables: broadcast variables and accumulator variables.
You only need to use shared variables explicitly to optimize your code in certain situations.</p></div>
<div class="paragraph"><p>In this presentation, we will discuss accumulator variables.</p></div>
</div>
</div>
</section>
<section class="slide" id="accumulator-variables">
<h2>Accumulator Variables</h2>
<div class="paragraph"><p><strong>Definition and use cases</strong></p></div>
<div class="paragraph"><p><em><strong>Definition</strong></em></p></div>
<div class="verseblock">
<pre class="content">Accumulator variables are aggregate-only variables whose values can be
updated using an associative operation by tasks running on each node in a cluster
and whose final aggregated values can be accessed in client&#8217;s driver program</pre>
</div>
<div class="paragraph"><p><br /></p></div>
<div class="paragraph"><p><em><strong>Main use cases</strong></em></p></div>
<div class="ulist">
<ul>
<li>Counting and summation</li>
<li>Application needs to compute multiple aggregates on the same dataset</li>
<li>Application needs custom aggregation not supported by existing Spark operations</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Accumulator variables are aggregate-only variables whose values can be
updated using an associative operation by tasks running on each node in a cluster
and whose final aggregated values can be accessed in client&#8217;s driver program.</p></div>
<div class="paragraph"><p>Typical applications of accumulators are counting and summation.
The main reasons to use accumulator variables include:</p></div>
<div class="ulist">
<ul>
<li>Application needs to compute multiple aggregates on the same dataset</li>
<li>Application needs custom aggregation not supported by existing Spark operations</li>
</ul>
</div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>API</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Statement</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">val <em>accumulatorVar</em> = <em>sc</em>.<em>accumulator</em>(<em>initialValue</em>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Declare and initialize accumulator variable <em>accumulatorVar</em> with <em>initialValue</em> of a numeric type
using method <em>accumulator</em> of the Spark Context object <em>sc</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>accumulatorVar</em> += 1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Update <em>accumulatorVar</em> in a Spark action, such as <em>foreach</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>accumulatorVar</em>.<em>value</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Access final aggregated <em>value</em> of accumulator variable <em>accumulatorVar</em> in a client program.</p></td>
</tr>
</tbody>
</table>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Accumulator variables are very simple to create and use.</p></div>
<div class="paragraph"><p>It is worth noting that while Spark provides the build-in support for accumulator variables of
numeric types, it is also possible to extend this functionality to other types and even support situations
when an aggregate value is of a different type from input value types,
such us aggregating values into a collection.</p></div>
</div>
</div>
</section>
<section class="slide" id="example-2">
<h2>Example</h2>
<div class="paragraph"><p><strong>Not using accumulator variables</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="favorite movies" src="images/spark/optimization-techniques/accumulator-variables/favorite_movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movieRatings = sc.cassandraTable("killr_video","favorite_movies")
                     .select("rating")
                     .filter(row =&gt; row.getFloatOption("rating").isDefined)
                     .map(row =&gt; row.getFloat("rating"))
                     .cache

val numRatings   = movieRatings.count
val sumRatings   = movieRatings.sum
val avgRating    = sumRatings / numRatings

println(f"$avgRating%1.1f")</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To demonstrate when and how to use accumulator variables, let us explore this example first. We are
computing an average of all movie ratings stored in a Cassandra table. We are not using
accumulator variables yet.</p></div>
<div class="paragraph"><p>First, we retrieve all movie ratings from Cassandra table <em>favorite_movies</em> into RDD <em>movieRatings</em>,
<em>map</em> Cassandra rows with non-<em>null</em> ratings to just rating numeric values,
and <em>cache</em> the resulting RDD because it is used with multiple actions.</p></div>
<div class="paragraph"><p>Next, we are calling actions <em>count</em> and <em>sum</em> on RDD <em>movieRatings</em> to compute the number of ratings
and their total sum and store results as values <em>numRatings</em> and <em>sumRatings</em>, respectively.</p></div>
<div class="paragraph"><p>Finally, we are computing and printing an average rating locally (outside of Spark).</p></div>
<div class="paragraph"><p>The computation logic is quite simple and this code runs with no errors. However, notice
that we are using two actions on the same RDD and, even though we are caching the RDD to avoid re-computation,
there will be separate tasks scheduled to compute <em>count</em> and <em>sum</em>. It is like we are doing
two passes over the same dataset. With accumulator variables, we only need one pass!</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Using accumulator variables</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="favorite movies" src="images/spark/optimization-techniques/accumulator-variables/favorite_movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val numRatings = sc.accumulator(0)
val sumRatings = sc.accumulator(0.0)

sc.cassandraTable("killr_video","favorite_movies")
  .select("rating")
  .filter(row =&gt; row.getFloatOption("rating").isDefined)
  .foreach{row =&gt; numRatings += 1; sumRatings += row.getFloat("rating")}

val avgRating = sumRatings.value / numRatings.value

println(f"$avgRating%1.1f")</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>This code is now optimized!
Both <em>numRating</em> and <em>sumRatings</em> are accumulator variables with initial values of 0 (of type <em>Int</em>) and
0.0 (of type <em>Double</em>), respectively. These variables are updated in the <em>foreach</em> action to do
counting and summation. The average is then computed locally (outside of Spark) by accessing final aggregate values.</p></div>
<div class="paragraph"><p>What we achieved:</p></div>
<div class="ulist">
<ul>
<li>We only use one action to do all aggregation and therefore, have to do only one pass over the dataset
retrieved from Cassandra.</li>
<li>We no longer need caching, which is an additional performance and memory-saving benefit.</li>
</ul>
</div>
</div>
</div>
</section>
<section class="slide" id="spark-optimization-techniques-rdd-persistence">
<h2>The Challenge: Suboptimal Code</h2>
<div class="paragraph"><p><strong>Computing percentages of comedy movies released in 2014 and 2013</strong></p></div>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/optimization-techniques/rdd-persistence/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies")
               .select("release_year","genres")

val movies2014 = movies.filter(row =&gt; row.getInt("release_year") == 2014)
val total2014  = movies2014.count
val comedy2014 = movies2014.filter(row =&gt; row.getSet[String]("genres")
                                          contains "Comedy").count
val percentage2014 = 100.0 * comedy2014 / total2014

val movies2013 = movies.filter(row =&gt; row.getInt("release_year") == 2013)
val total2013  = movies2013.count
val comedy2013 = movies2013.filter(row =&gt; row.getSet[String]("genres")
                                          contains "Comedy").count
val percentage2013 = 100.0 * comedy2013 / total2013</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Study this simple program that serves as our running example in this presentation.</p></div>
<div class="paragraph"><p>Its goal is to analyze how the percentage of comedy movies from 2014 compares to the
percentage of comedy movies from 2013. Did we have more laughter in 2014 or in 2013?</p></div>
<div class="paragraph"><p>We first retrieve all movies from Cassandra table <em>movies</em>
and only keep information about <em>release_year</em> and <em>genres</em> for each movie.</p></div>
<div class="paragraph"><p>We then apply <em>filter</em> based on <em>release_year</em> == 2014 and <em>count</em> the total number
of movies release in 2014. We use another <em>filter</em> to only keep comedy movies from 2014 and we <em>count</em> them.</p></div>
<div class="paragraph"><p>Given the two counts, <em>total2014</em> and <em>comedy2014</em>, it is straightforward to compute the percentage of comedy
movies from 2014.</p></div>
<div class="paragraph"><p>We do the same for movies from 2013.</p></div>
<div class="paragraph"><p>Like we said, this program is simple and it works. However, it is suboptimal as we show in the following slides.
The challenge is to optimize this code to run faster!</p></div>
</div>
</div>
</section>
<section class="slide" id="dag-of-operations">
<h2>DAG of Operations</h2>
<div class="imageblock center">
<div class="content">
<img alt="dag" src="images/spark/optimization-techniques/rdd-persistence/dag.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>To understand how our program is executed by Spark, let us look at the
Directed Acyclic Graph (DAG) of operations (e.g., transformations and actions).
In this case, the DAG is really a tree because we are not using any binary operations.</p></div>
<div class="paragraph"><p>Think about this DAG as a logical evaluation plan for our computation.
Notice that leaf nodes ALWAYS represent results returned by actions because only actions trigger computation.
Transformations are evaluated lazily and will not do actual computation until an action is seen.</p></div>
<div class="paragraph"><p>A physical evaluation plan may however be different.</p></div>
</div>
</div>
</section>
<section class="slide" id="stages-of-computation">
<h2>Stages of Computation</h2>
<div class="paragraph"><p><strong>Stage 1</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stage1" src="images/spark/optimization-techniques/rdd-persistence/stage1.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Spark will evaluate our program in stages. Here is the first stage.</p></div>
<div class="paragraph"><p>When Spark sees the first <em>count</em> action, it starts evaluation. In particular, to return the result to a client,
Spark needs to compute all intermediate RDDs: retrieve data from Cassandra into RDD <em>movies</em>, apply <em>filter</em> to get new RDD
<em>movies2014</em> and <em>count</em> its elements.</p></div>
<div class="paragraph"><p>What is VERY IMPORTANT to understand is that intermediate data products, such RDDs <em>movies</em> and <em>movies2014</em>,
are not completely materialized in memory. Spark only needs one partition to schedule a computational task and once
the task completes, memory can be reused for another partition that will be processed by another task.
In other words, intermediate results will be lost.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Stages 1 and 2</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stage2" src="images/spark/optimization-techniques/rdd-persistence/stage2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Once Spark finishes Stage 1, resources are freed to deal with our second <em>count</em> action.
This is Stage 2. While tasks in each stage are executed in parallel, stages are executed sequentially.</p></div>
<div class="paragraph"><p>To compute the result of the second <em>count</em>, Spark has to compute three intermediate data products. We
have seen RDDs <em>movies</em> and <em>movies2014</em> before, in Stage 1, but their data is lost and has to be recomputed
again in Stage 2. The third RDD is unnamed: we do not have a <em>val</em> to refer to it in the program.</p></div>
<div class="paragraph"><p>Wow! Stage 2 does not reuse anything from Stage 1!</p></div>
<div class="paragraph"><p>Note: "(1)" and "(2)" on the diagram denote first and second time a dataset is computed, respectively. For
example, data for RDDs <em>movies</em> and <em>movies2014</em> is computed for the second time in Stage 2.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Stages 1, 2, and 3</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stage3" src="images/spark/optimization-techniques/rdd-persistence/stage3.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Stage 3. You get the idea. RDD <em>movies</em> is recomputed again.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Stages 1, 2, 3, and 4</strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="stage4" src="images/spark/optimization-techniques/rdd-persistence/stage4.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Final Stage 4. How inefficient! A lot of redundant re-computation!</p></div>
</div>
</div>
</section>
<section class="slide" id="the-challenge-summary">
<h2>The Challenge: Summary</h2>
<div class="paragraph"><p>Efficiency issues:</p></div>
<div class="ulist">
<ul>
<li>Reading the same data from Cassandra into RDD <em>movies</em> four times</li>
<li>Recomputing RDD <em>movies2014</em> twice</li>
<li>Recomputing RDD <em>movies2013</em> twice</li>
</ul>
</div>
<div class="paragraph"><p>Take aways:</p></div>
<div class="ulist">
<ul>
<li>Recomputing an RDD multiple times due to multiple actions on the RDD or its derivatives</li>
<li>Need a way to materialize and reuse an RDD after it is computed for the first time</li>
</ul>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Indeed, our code is suboptimal.
We can do better!</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-persistence">
<h2>RDD Persistence</h2>
<div class="paragraph"><p><strong>One of the most important optimizations in Spark for iterative algorithms and interactive computation!</strong></p></div>
<div class="ulist">
<ul>
<li>You can instruct Spark to cache or persist any RDD in your program</li>
<li>Persisted RDD is kept in memory (by default) once it is computed for the first time</li>
<li>Persisted RDD is reused by other operations that require the same dataset</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>The persistence mechanism should be used to avoid recomputing the same dataset multiple times.
With respect to a DAG of operations, any RDD that is a common ancestor of two or more leaf nodes resulting from
actions is a good candidate for the persistence optimization.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>The mechanism that allows us to materialize and reuse an RDD in Spark is called <em>RDD persistence</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="rdd-persistence-api">
<h2>RDD Persistence API</h2>
<div class="paragraph"><p><strong>Any RDD, including a Cassandra RDD, can be cached or persisted</strong></p></div>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Transformation</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>persist</strong>([<em>storageLevel</em>])</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persists the source RDD according to a storage level specified by the optional parameter. The default storage level is
<em>org.apache.spark.storage.StorageLevel.MEMORY_ONLY</em>, which prescribes persisting
RDD elements as deserialized Java objects in the JVM.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>cache</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Same as <em>persist</em>() or <em>persist</em>(<em>StorageLevel.MEMORY_ONLY</em>).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>unpersist</strong>()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unpersists the source RDD (manually). It is safe to not use this transformation
because Spark automatically monitors and unpersists least-recently-used RDD partitions.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>When using <em>cache</em>() or <em>persist</em>(), if an RDD does not fit into memory,
some partitions will not be cached and will be recomputed on the fly when needed.</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here are the three transformations of the RDD Persistence API.</p></div>
<div class="paragraph"><p>More storage levels for <em>persist</em> are discussed on the next slide.</p></div>
<div class="paragraph"><p><em>cache</em> is a convenient synonym of <em>persist</em> with the default storage level.</p></div>
<div class="paragraph"><p>You almost never need to use <em>unpersist</em> explicitly unless there is a specific need
in your application to force Spark to unpersist an RDD immediately.</p></div>
</div>
</div>
</section>
<section class="slide" id="storage-levels">
<h2>Storage Levels</h2>
<table class="tableblock frame-all grid-all" style="width:100%">
<colgroup>
<col style="width:25%" />
<col style="width:75%" />
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Storage Level</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_ONLY</em> <em>MEMORY_ONLY_SER</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting RDD elements as <em>deserialized</em> or <em>serialized</em> Java objects in the JVM, respectively. Partitions
that do not fit into memory are not cached and recomputed when needed.
<em>MEMORY_ONLY_SER</em> is more space-efficient but more CPU-intensive than <em>MEMORY_ONLY</em>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_AND_DISK</em> <em>MEMORY_AND_DISK_SER</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting RDD elements as <em>deserialized</em> or <em>serialized</em> Java objects in the JVM, respectively. Partitions
that do not fit into memory are spilled to disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>DISK_ONLY</em></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting RDD partitions on disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>MEMORY_ONLY_2</em>, <em>MEMORY_AND_DISK_2</em>, &#8230;&#8203;</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Same as the respective storage levels above, but with replication on two nodes in a cluster.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>OFF_HEAP</em> (experimental)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Persisting the source RDD in <em>serialized</em> format in <em>Tachyon</em> (a memory-centric distributed storage system).</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph"><p>In some cases, recomputing partitions may be faster than reading persisted partitions from disk!</p></div>
</td>
</tr>
</table>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Study the different storage level possibilities for <em>persist</em>.</p></div>
</div>
</div>
</section>
<section class="slide" id="solving-the-challenge">
<h2>Solving the Challenge</h2>
<div class="paragraph"><p><strong>Caching RDD <em>movies</em></strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="cache1" src="images/spark/optimization-techniques/rdd-persistence/cache1.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Let us use the Persistence API to optimize our program.</p></div>
<div class="paragraph"><p>This diagram shows the four stages of computation after
we <em>cache</em> RDD <em>movies</em>. As a result, RDD <em>movies</em> is computed
and cached once during Stage 1 and is reused in the remaining three stages.</p></div>
<div class="paragraph"><p>We still have redundant computation for RDDs <em>movies2014</em> and <em>movies2013</em>.</p></div>
</div>
</div>
<div style="page-break-after: always"></div>
<div class="paragraph"><p><strong>Caching RDDs <em>movies2014</em> and <em>movies2013</em></strong></p></div>
<div class="imageblock center">
<div class="content">
<img alt="cache2" src="images/spark/optimization-techniques/rdd-persistence/cache2.svg" />
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>After we <em>cache</em> RDDs <em>movies2014</em> and <em>movies2013</em>, we have
optimal computation.</p></div>
</div>
</div>
</section>
<section class="slide" id="final-solution">
<h2>Final Solution</h2>
<div class="imageblock right" style="float: right">
<div class="content">
<img alt="movies" src="images/spark/optimization-techniques/rdd-persistence/movies.svg" />
</div>
</div>
<div class="listingblock left">
<div class="content">
<pre class="CodeRay"><code>val movies = sc.cassandraTable("killr_video","movies")
               .select("release_year","genres")
               .cache

val movies2014 = movies.filter(row =&gt; row.getInt("release_year") == 2014)
                       .cache
val total2014  = movies2014.count
val comedy2014 = movies2014.filter(row =&gt; row.getSet[String]("genres")
                                          contains "Comedy").count
val percentage2014 = 100.0 * comedy2014 / total2014

val movies2013 = movies.filter(row =&gt; row.getInt("release_year") == 2013)
                       .cache
val total2013  = movies2013.count
val comedy2013 = movies2013.filter(row =&gt; row.getSet[String]("genres")
                                          contains "Comedy").count
val percentage2013 = 100.0 * comedy2013 / total2013</code></pre>
</div>
</div>
<div class="openblock notes">
<div class="content">
<div class="paragraph"><p>Here is our final solution! We only added caching for the three RDDs
to make our code run faster.</p></div>
<div class="paragraph"><p>As a final note, if you execute this code in your application and point your
browser to the Spark Application UI at port 4040, you will be able to see the
four stages of computation and storage information for persisted RDDs.
Have fun!</p></div>
</div>
</div>
</section>
<div aria-role="navigation">
<a class="deck-prev-link" href="#" title="Previous">
<i class="icon-chevron-with-circle-left"></i>
</a>
<a class="deck-next-link" href="#" title="Next">
<i class="icon-chevron-with-circle-right"></i>
</a>
</div>
</div>
<script src="deck.js/jquery.min.js"></script>
<script src="deck.js/d3.v2.js"></script>
<script src="deck.js/jquery-ui.min.js"></script>
<script src="deck.js/core/deck.core.js"></script>
<script src="deck.js/extensions/scale/deck.scale.js"></script>
<script src="deck.js/extensions/navigation/deck.navigation.js"></script>
<script src="deck.js/extensions/split/deck.split.js"></script>
<script src="deck.js/extensions/animation/deck.animation.js"></script>
<script src="deck.js/extensions/deck.js-notes/deck.notes.js"></script>
<script src="deck.js/extensions/goto/deck.goto.js"></script>
<script src="deck.js/extensions/clone/deck.clone.js"></script>
<script src="deck.js/extensions/svg/svg.min.js"></script>
<script src="js/course.js"></script>
<footer>
<div class="flex-element deck-course">
<p>&copy; 2016 DataStax. Use only with permission. &bull;
<span class="course-title">Optimization is not Always Premature</span></p>
</div>
<div class="flex-element deck-brand">
<a href="http://academy.datastax.com" target="blank">DataStax Academy</a>
</div>
<div class="deck-progressbar">
<span></span>
</div>
</footer>
<script type="text/javascript">
  //<![CDATA[
    (function($, deck, undefined) {
      $.deck.defaults.keys['previous'] = [8, 33, 37, 39];
      $.deck.defaults.keys['next'] = [13, 32, 34, 39];
    
      $.extend(true, $[deck].defaults, {
          countNested: false
      });
    
      $.deck('.slide');
      $.deck('disableScale');
    })(jQuery, 'deck');
  //]]>
</script>
<script type="text/javascript">
  //<![CDATA[
    $(document).bind('deck.change', function(event, from, to) {
      var width = to / ($.deck('getSlides').length - 1) * 100;
      $('.deck-progressbar span').css('width', width + '%');
    });
  //]]>
</script>
</body>
</html>